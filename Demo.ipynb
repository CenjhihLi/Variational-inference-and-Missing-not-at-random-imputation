{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font size = 6>**\n",
    "# This is the `Demo.ipynb` file\n",
    "\n",
    "I will demonstrate how to use my classes and functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
    "seed = 1\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font size = 4>**\n",
    "# The `UCIDatasets` loader in `/utils/dataframe.py `\n",
    "\n",
    "`reference: https://gist.github.com/martinferianc/db7615c85d5a3a71242b4916ea6a14a2`\n",
    "\n",
    "Note that the output `train` or `test` is pytorch.Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['housing', 'concrete', 'energy', 'power', 'redwine', 'whitewine', 'yacht']\n"
     ]
    }
   ],
   "source": [
    "from utils.dataframe import UCIDatasets, datalist\n",
    "print(datalist)\n",
    "data = UCIDatasets(\"housing\")\n",
    "train = data.get_split( load=\"train\") #pytorch.dataset\n",
    "test = data.get_split( load=\"test\")\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "train_loader = data.get_split_dataloader(load = \"train\", batch_size = 16) #pytorch.dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font size = 4>**\n",
    "# The `imputer` in `model/imputer.py`.\n",
    "\n",
    "I assemble several common methods, some detail can see my code and the documentary of sklearn.impute. \n",
    "\n",
    "I have set up some parameters. change the parameters via `par_setting( par_dict )` function.\n",
    "\n",
    "The input dictionary should be like `__getParvalue__()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mean', 'median', 'most_frequent', 'mice', 'missForest', 'knn']\n",
      "\n",
      "\n",
      "Set up parameters via impObject.par_setting( par_dict ) and retrain the model,     the par_dict should be like the following structure:\n",
      " {'missing_values': nan, 'max_iter': 10, 'random_state': 0, 'n_estimators': 100, 'n_neighbors': 3, 'metric': 'nan_euclidean'} \n",
      "\n",
      "for instance: impObject.par_setting( { 'max_iter': 10 } ) \n",
      "\n",
      "Each par corresponding to method is as the following:\n",
      "where SimpleImputer includes ['mean', 'median', 'most_frequent']\n"
     ]
    }
   ],
   "source": [
    "from model.imputer import imputer, method_list\n",
    "import numpy as np\n",
    "print(method_list)\n",
    "X = np.random.rand(100*20).reshape(100,20)\n",
    "X[51:, 11:] = np.nan\n",
    "\n",
    "impObject = imputer(X, method = 'mice')\n",
    "impObject.train()\n",
    "imp = impObject.imp\n",
    "imp.transform(X).shape \n",
    "print('\\n\\nSet up parameters via impObject.par_setting( par_dict ) and retrain the model, \\\n",
    "    the par_dict should be like the following structure:\\n {} \\n'.format(impObject.get_Parvalue()))\n",
    "print('for instance: impObject.par_setting( { \\'max_iter\\': 10 } ) ')\n",
    "print('\\nEach par corresponding to method is as the following:')\n",
    "impObject.get_Parlist()\n",
    "print('where SimpleImputer includes [\\'mean\\', \\'median\\', \\'most_frequent\\']')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font size = 4>**\n",
    "# This part is for `MIWAE` in `/model/MIWAE.py` and `trainer` in `/utils/trainer.py`.\n",
    "\n",
    "`http://proceedings.mlr.press/v97/mattei19a/mattei19a.pdf (ICML, 2019)`.\n",
    "\n",
    "`MIWAE` is a pytorch model\n",
    "\n",
    "Might use `trainer` to train it.\n",
    "\n",
    "Loss function are like `loss(self, outdic, indic)` in which outdic and indic are input and output\n",
    "\n",
    "For `MIWAE`, \n",
    "\n",
    "`indic = {'x': x , 'm': m }` where `x` is dataset and `m` is missing indicator.\n",
    "\n",
    "`output = {'lpxz': lpxz , 'lqzx': lqzx , 'lpz': lpz }`  means `l`og loss for `p`(`x` | `z`), `q`(`z` | `x`), `p`(`z`)\n",
    "\n",
    "In MIWAE loss `self.MIWAE_ELBO(outdic, indic = None)` the indic is not required.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Es torch.Size([16, 20, 20])\n",
      "Esx torch.Size([16, 20, 21])\n",
      "Esxr torch.Size([320, 21])\n",
      "h torch.Size([320, 20])\n",
      "hr torch.Size([16, 20, 20])\n",
      "hz torch.Size([16, 20, 20])\n",
      "g torch.Size([16, 20])\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                   [-1, 20]             440\n",
      "            Linear-2                  [-1, 100]           2,100\n",
      "              Tanh-3                  [-1, 100]               0\n",
      "            Linear-4                  [-1, 100]          10,100\n",
      "              Tanh-5                  [-1, 100]               0\n",
      "            Linear-6                   [-1, 50]           5,050\n",
      "            Linear-7                   [-1, 50]           5,050\n",
      "            Linear-8               [-1, 5, 100]           5,100\n",
      "            Linear-9               [-1, 5, 100]          10,100\n",
      "           Linear-10                [-1, 5, 20]           2,020\n",
      "           Linear-11                [-1, 5, 20]           2,020\n",
      "================================================================\n",
      "Total params: 41,980\n",
      "Trainable params: 41,980\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.01\n",
      "Params size (MB): 0.16\n",
      "Estimated Total Size (MB): 0.17\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from model.MIWAE import MIWAE\n",
    "from utils.trainer import VAE_trainer\n",
    "from utils.dataframe import UCIDatasets, datalist\n",
    "data = UCIDatasets(\"whitewine\")\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "train_loader = data.get_split_dataloader(load = \"train\", batch_size = 16) #pytorch.dataloader\n",
    "test_loader = data.get_split_dataloader(load = \"test\", batch_size = 16) #pytorch.dataloader\n",
    "model = MIWAE(data_dim = 20, n_samples=5, permutation_invariance=True)\n",
    "trainer = VAE_trainer(model = model, train_loader = train_loader, test_loader = test_loader)\n",
    "trainer.model_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font size = 4>**\n",
    "# A simple example\n",
    "\n",
    "Here is a simple example using `VAE_trainer` class from `/utils/trainer.py` to train `VAE` on `MNIST`.\n",
    "\n",
    "First, we may set some hyperparameters.\n",
    "\n",
    "Then read the training data and validation data (`train_loader` structure) and put them into trainer.\n",
    "\n",
    "The trainer will use `VAE_loss` automatically for `VAE` model.\n",
    "\n",
    "I will use this trainer for all `VAE` like models in this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 51 [0/60000 (0%)]\tLoss: 99.689117\n",
      "Train Epoch: 51 [1280/60000 (2%)]\tLoss: 103.431137\n",
      "Train Epoch: 51 [2560/60000 (4%)]\tLoss: 104.766655\n",
      "Train Epoch: 51 [3840/60000 (6%)]\tLoss: 99.233307\n",
      "Train Epoch: 51 [5120/60000 (9%)]\tLoss: 101.182518\n",
      "Train Epoch: 51 [6400/60000 (11%)]\tLoss: 102.850739\n",
      "Train Epoch: 51 [7680/60000 (13%)]\tLoss: 101.406624\n",
      "Train Epoch: 51 [8960/60000 (15%)]\tLoss: 100.912262\n",
      "Train Epoch: 51 [10240/60000 (17%)]\tLoss: 102.272636\n",
      "Train Epoch: 51 [11520/60000 (19%)]\tLoss: 103.237030\n",
      "Train Epoch: 51 [12800/60000 (21%)]\tLoss: 99.779877\n",
      "Train Epoch: 51 [14080/60000 (23%)]\tLoss: 100.001404\n",
      "Train Epoch: 51 [15360/60000 (26%)]\tLoss: 107.487274\n",
      "Train Epoch: 51 [16640/60000 (28%)]\tLoss: 100.886490\n",
      "Train Epoch: 51 [17920/60000 (30%)]\tLoss: 103.306786\n",
      "Train Epoch: 51 [19200/60000 (32%)]\tLoss: 100.287735\n",
      "Train Epoch: 51 [20480/60000 (34%)]\tLoss: 103.765167\n",
      "Train Epoch: 51 [21760/60000 (36%)]\tLoss: 98.166534\n",
      "Train Epoch: 51 [23040/60000 (38%)]\tLoss: 101.741867\n",
      "Train Epoch: 51 [24320/60000 (41%)]\tLoss: 99.962921\n",
      "Train Epoch: 51 [25600/60000 (43%)]\tLoss: 102.475372\n",
      "Train Epoch: 51 [26880/60000 (45%)]\tLoss: 100.807495\n",
      "Train Epoch: 51 [28160/60000 (47%)]\tLoss: 103.751839\n",
      "Train Epoch: 51 [29440/60000 (49%)]\tLoss: 101.524963\n",
      "Train Epoch: 51 [30720/60000 (51%)]\tLoss: 97.835808\n",
      "Train Epoch: 51 [32000/60000 (53%)]\tLoss: 102.529854\n",
      "Train Epoch: 51 [33280/60000 (55%)]\tLoss: 101.557816\n",
      "Train Epoch: 51 [34560/60000 (58%)]\tLoss: 103.308334\n",
      "Train Epoch: 51 [35840/60000 (60%)]\tLoss: 97.717880\n",
      "Train Epoch: 51 [37120/60000 (62%)]\tLoss: 102.811386\n",
      "Train Epoch: 51 [38400/60000 (64%)]\tLoss: 102.140579\n",
      "Train Epoch: 51 [39680/60000 (66%)]\tLoss: 98.035789\n",
      "Train Epoch: 51 [40960/60000 (68%)]\tLoss: 104.371460\n",
      "Train Epoch: 51 [42240/60000 (70%)]\tLoss: 103.376175\n",
      "Train Epoch: 51 [43520/60000 (72%)]\tLoss: 104.932373\n",
      "Train Epoch: 51 [44800/60000 (75%)]\tLoss: 101.941223\n",
      "Train Epoch: 51 [46080/60000 (77%)]\tLoss: 105.335197\n",
      "Train Epoch: 51 [47360/60000 (79%)]\tLoss: 102.158844\n",
      "Train Epoch: 51 [48640/60000 (81%)]\tLoss: 100.041145\n",
      "Train Epoch: 51 [49920/60000 (83%)]\tLoss: 100.979431\n",
      "Train Epoch: 51 [51200/60000 (85%)]\tLoss: 101.817467\n",
      "Train Epoch: 51 [52480/60000 (87%)]\tLoss: 100.590324\n",
      "Train Epoch: 51 [53760/60000 (90%)]\tLoss: 102.941391\n",
      "Train Epoch: 51 [55040/60000 (92%)]\tLoss: 98.676582\n",
      "Train Epoch: 51 [56320/60000 (94%)]\tLoss: 106.054207\n",
      "Train Epoch: 51 [57600/60000 (96%)]\tLoss: 101.895187\n",
      "Train Epoch: 51 [58880/60000 (98%)]\tLoss: 105.019066\n",
      "====> Epoch: 51 Average loss: 101.6453\n",
      "====> Test set loss: 101.7710\n",
      "Train Epoch: 52 [0/60000 (0%)]\tLoss: 102.169739\n",
      "Train Epoch: 52 [1280/60000 (2%)]\tLoss: 103.011726\n",
      "Train Epoch: 52 [2560/60000 (4%)]\tLoss: 105.368484\n",
      "Train Epoch: 52 [3840/60000 (6%)]\tLoss: 105.302391\n",
      "Train Epoch: 52 [5120/60000 (9%)]\tLoss: 104.988525\n",
      "Train Epoch: 52 [6400/60000 (11%)]\tLoss: 101.651978\n",
      "Train Epoch: 52 [7680/60000 (13%)]\tLoss: 103.728104\n",
      "Train Epoch: 52 [8960/60000 (15%)]\tLoss: 102.348160\n",
      "Train Epoch: 52 [10240/60000 (17%)]\tLoss: 104.877380\n",
      "Train Epoch: 52 [11520/60000 (19%)]\tLoss: 101.100052\n",
      "Train Epoch: 52 [12800/60000 (21%)]\tLoss: 98.888451\n",
      "Train Epoch: 52 [14080/60000 (23%)]\tLoss: 100.670212\n",
      "Train Epoch: 52 [15360/60000 (26%)]\tLoss: 99.660263\n",
      "Train Epoch: 52 [16640/60000 (28%)]\tLoss: 103.591293\n",
      "Train Epoch: 52 [17920/60000 (30%)]\tLoss: 96.341492\n",
      "Train Epoch: 52 [19200/60000 (32%)]\tLoss: 97.905128\n",
      "Train Epoch: 52 [20480/60000 (34%)]\tLoss: 101.395599\n",
      "Train Epoch: 52 [21760/60000 (36%)]\tLoss: 98.791550\n",
      "Train Epoch: 52 [23040/60000 (38%)]\tLoss: 99.417778\n",
      "Train Epoch: 52 [24320/60000 (41%)]\tLoss: 105.070679\n",
      "Train Epoch: 52 [25600/60000 (43%)]\tLoss: 103.435097\n",
      "Train Epoch: 52 [26880/60000 (45%)]\tLoss: 103.944893\n",
      "Train Epoch: 52 [28160/60000 (47%)]\tLoss: 99.101295\n",
      "Train Epoch: 52 [29440/60000 (49%)]\tLoss: 97.676903\n",
      "Train Epoch: 52 [30720/60000 (51%)]\tLoss: 101.960327\n",
      "Train Epoch: 52 [32000/60000 (53%)]\tLoss: 103.501678\n",
      "Train Epoch: 52 [33280/60000 (55%)]\tLoss: 102.770103\n",
      "Train Epoch: 52 [34560/60000 (58%)]\tLoss: 93.609131\n",
      "Train Epoch: 52 [35840/60000 (60%)]\tLoss: 103.461563\n",
      "Train Epoch: 52 [37120/60000 (62%)]\tLoss: 99.187988\n",
      "Train Epoch: 52 [38400/60000 (64%)]\tLoss: 101.221886\n",
      "Train Epoch: 52 [39680/60000 (66%)]\tLoss: 101.315292\n",
      "Train Epoch: 52 [40960/60000 (68%)]\tLoss: 103.663193\n",
      "Train Epoch: 52 [42240/60000 (70%)]\tLoss: 102.529434\n",
      "Train Epoch: 52 [43520/60000 (72%)]\tLoss: 101.679840\n",
      "Train Epoch: 52 [44800/60000 (75%)]\tLoss: 100.731308\n",
      "Train Epoch: 52 [46080/60000 (77%)]\tLoss: 101.169922\n",
      "Train Epoch: 52 [47360/60000 (79%)]\tLoss: 101.699234\n",
      "Train Epoch: 52 [48640/60000 (81%)]\tLoss: 100.461029\n",
      "Train Epoch: 52 [49920/60000 (83%)]\tLoss: 102.906021\n",
      "Train Epoch: 52 [51200/60000 (85%)]\tLoss: 101.869247\n",
      "Train Epoch: 52 [52480/60000 (87%)]\tLoss: 102.393135\n",
      "Train Epoch: 52 [53760/60000 (90%)]\tLoss: 98.207634\n",
      "Train Epoch: 52 [55040/60000 (92%)]\tLoss: 102.365883\n",
      "Train Epoch: 52 [56320/60000 (94%)]\tLoss: 101.104111\n",
      "Train Epoch: 52 [57600/60000 (96%)]\tLoss: 98.011322\n",
      "Train Epoch: 52 [58880/60000 (98%)]\tLoss: 101.931931\n",
      "====> Epoch: 52 Average loss: 101.6796\n",
      "====> Test set loss: 101.8767\n",
      "Train Epoch: 53 [0/60000 (0%)]\tLoss: 103.118668\n",
      "Train Epoch: 53 [1280/60000 (2%)]\tLoss: 102.135986\n",
      "Train Epoch: 53 [2560/60000 (4%)]\tLoss: 95.534485\n",
      "Train Epoch: 53 [3840/60000 (6%)]\tLoss: 104.947380\n",
      "Train Epoch: 53 [5120/60000 (9%)]\tLoss: 97.336594\n",
      "Train Epoch: 53 [6400/60000 (11%)]\tLoss: 96.759491\n",
      "Train Epoch: 53 [7680/60000 (13%)]\tLoss: 98.588120\n",
      "Train Epoch: 53 [8960/60000 (15%)]\tLoss: 101.163742\n",
      "Train Epoch: 53 [10240/60000 (17%)]\tLoss: 100.622009\n",
      "Train Epoch: 53 [11520/60000 (19%)]\tLoss: 101.689560\n",
      "Train Epoch: 53 [12800/60000 (21%)]\tLoss: 104.725380\n",
      "Train Epoch: 53 [14080/60000 (23%)]\tLoss: 106.181183\n",
      "Train Epoch: 53 [15360/60000 (26%)]\tLoss: 104.706924\n",
      "Train Epoch: 53 [16640/60000 (28%)]\tLoss: 104.665329\n",
      "Train Epoch: 53 [17920/60000 (30%)]\tLoss: 96.452072\n",
      "Train Epoch: 53 [19200/60000 (32%)]\tLoss: 105.103447\n",
      "Train Epoch: 53 [20480/60000 (34%)]\tLoss: 103.517090\n",
      "Train Epoch: 53 [21760/60000 (36%)]\tLoss: 103.346283\n",
      "Train Epoch: 53 [23040/60000 (38%)]\tLoss: 100.213806\n",
      "Train Epoch: 53 [24320/60000 (41%)]\tLoss: 101.396774\n",
      "Train Epoch: 53 [25600/60000 (43%)]\tLoss: 101.085571\n",
      "Train Epoch: 53 [26880/60000 (45%)]\tLoss: 100.206146\n",
      "Train Epoch: 53 [28160/60000 (47%)]\tLoss: 101.436852\n",
      "Train Epoch: 53 [29440/60000 (49%)]\tLoss: 103.084740\n",
      "Train Epoch: 53 [30720/60000 (51%)]\tLoss: 101.377441\n",
      "Train Epoch: 53 [32000/60000 (53%)]\tLoss: 101.302902\n",
      "Train Epoch: 53 [33280/60000 (55%)]\tLoss: 99.221237\n",
      "Train Epoch: 53 [34560/60000 (58%)]\tLoss: 101.238190\n",
      "Train Epoch: 53 [35840/60000 (60%)]\tLoss: 101.625443\n",
      "Train Epoch: 53 [37120/60000 (62%)]\tLoss: 98.852150\n",
      "Train Epoch: 53 [38400/60000 (64%)]\tLoss: 103.979820\n",
      "Train Epoch: 53 [39680/60000 (66%)]\tLoss: 98.916962\n",
      "Train Epoch: 53 [40960/60000 (68%)]\tLoss: 98.490662\n",
      "Train Epoch: 53 [42240/60000 (70%)]\tLoss: 99.171265\n",
      "Train Epoch: 53 [43520/60000 (72%)]\tLoss: 102.426147\n",
      "Train Epoch: 53 [44800/60000 (75%)]\tLoss: 103.543045\n",
      "Train Epoch: 53 [46080/60000 (77%)]\tLoss: 103.027664\n",
      "Train Epoch: 53 [47360/60000 (79%)]\tLoss: 101.298935\n",
      "Train Epoch: 53 [48640/60000 (81%)]\tLoss: 101.660339\n",
      "Train Epoch: 53 [49920/60000 (83%)]\tLoss: 103.978439\n",
      "Train Epoch: 53 [51200/60000 (85%)]\tLoss: 100.831421\n",
      "Train Epoch: 53 [52480/60000 (87%)]\tLoss: 100.069305\n",
      "Train Epoch: 53 [53760/60000 (90%)]\tLoss: 103.866913\n",
      "Train Epoch: 53 [55040/60000 (92%)]\tLoss: 100.436142\n",
      "Train Epoch: 53 [56320/60000 (94%)]\tLoss: 103.409775\n",
      "Train Epoch: 53 [57600/60000 (96%)]\tLoss: 99.417931\n",
      "Train Epoch: 53 [58880/60000 (98%)]\tLoss: 104.431648\n",
      "====> Epoch: 53 Average loss: 101.6055\n",
      "====> Test set loss: 101.7891\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import random\n",
    "import pathlib\n",
    "import gc\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "from model.VAE import VAE\n",
    "from utils.trainer import VAE_trainer\n",
    "from utils.experiment import fs_setup, check_training_file\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
    "seed = 1\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "######################################\n",
    "# settings\n",
    "batch_size=128\n",
    "max_epochs=53\n",
    "no_cuda = False\n",
    "seed=1\n",
    "log_interval=10\n",
    "cuda = not no_cuda and torch.cuda.is_available()\n",
    "torch.manual_seed(seed)\n",
    "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "\"\"\"\n",
    "Setup directory of Demo\n",
    "\"\"\"\n",
    "check_point = pathlib.Path('experiments') /'Demo' / 'VAE_ckpt.pth'\n",
    "expr_file = pathlib.Path('experiments') /'Demo' / 'VAE.npz'\n",
    "config = {'batch_size': batch_size }\n",
    "fs_setup('Demo', seed, config)\n",
    "###############\n",
    "# Read history loss file if exist\n",
    "###############\n",
    "train_file = check_training_file(expr_file)\n",
    "history, start_epoch = train_file['history'], train_file['start_epoch']\n",
    "if start_epoch >= max_epochs:\n",
    "    print('skipping {} (seed={})   start_epoch({}), num_of_epoch({})'.format('Demo', seed, start_epoch, max_epochs))\n",
    "del train_file\n",
    "gc.collect()\n",
    "###############\n",
    "# set kwargs for loading checkpoint if exist\n",
    "###############\n",
    "train_kwargs = {\n",
    "    'check_point': check_point, 'expr_file': expr_file, 'start_epoch': start_epoch, 'history': history,\n",
    "    } \n",
    "# end settings\n",
    "######################################\n",
    "\"\"\"\n",
    "Make data loader\n",
    "\"\"\"\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('./data', train=True, download=True,\n",
    "                   transform=transforms.ToTensor()),\n",
    "    batch_size=batch_size, shuffle=True, num_workers = 1, pin_memory = True )\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('./data', train=False, transform=transforms.transforms.ToTensor()),\n",
    "    batch_size=batch_size, shuffle=True, num_workers = 1, pin_memory = True)\n",
    "\"\"\"\n",
    "Train model\n",
    "\"\"\"\n",
    "model = VAE()\n",
    "trainer = VAE_trainer(model = model, train_loader = train_loader, test_loader= test_loader, \n",
    "            **config, **train_kwargs,)\n",
    "trainer.train(max_epochs = max_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font size = 4>**\n",
    "# Reproduce the experiment in the paper notMIWAE\n",
    "\n",
    "I will follow the setting in the paper https://arxiv.org/pdf/2006.12871.pdf\n",
    "\n",
    "The experiment is in `exp_imputation` (see `utils/experiment.py`)\n",
    "\n",
    "Use `UCIDatasets` to process the data\n",
    "\n",
    "Use `imputer` to train the `mean`, `MissForest`, `MICE`\n",
    "\n",
    "Use `VAE_trainer` to train the `miwae`, `notmiwae`\n",
    "\n",
    "Store check_points every 50 epochs, including `model.state_dict()` and `optimize.state_dict()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1001 [0/4897 (0%)]\tLoss: -13.796570\n",
      "Train Epoch: 1001 [160/4897 (3%)]\tLoss: -12.692852\n",
      "Train Epoch: 1001 [320/4897 (7%)]\tLoss: -17.271152\n",
      "Train Epoch: 1001 [480/4897 (10%)]\tLoss: -15.744910\n",
      "Train Epoch: 1001 [640/4897 (13%)]\tLoss: -15.577856\n",
      "Train Epoch: 1001 [800/4897 (16%)]\tLoss: -15.670021\n",
      "Train Epoch: 1001 [960/4897 (20%)]\tLoss: -17.834810\n",
      "Train Epoch: 1001 [1120/4897 (23%)]\tLoss: -17.109938\n",
      "Train Epoch: 1001 [1280/4897 (26%)]\tLoss: -17.342064\n",
      "Train Epoch: 1001 [1440/4897 (29%)]\tLoss: -15.976070\n",
      "Train Epoch: 1001 [1600/4897 (33%)]\tLoss: -15.522005\n",
      "Train Epoch: 1001 [1760/4897 (36%)]\tLoss: -13.761829\n",
      "Train Epoch: 1001 [1920/4897 (39%)]\tLoss: -17.198116\n",
      "Train Epoch: 1001 [2080/4897 (42%)]\tLoss: -19.258833\n",
      "Train Epoch: 1001 [2240/4897 (46%)]\tLoss: -14.971960\n",
      "Train Epoch: 1001 [2400/4897 (49%)]\tLoss: -10.205468\n",
      "Train Epoch: 1001 [2560/4897 (52%)]\tLoss: -19.019260\n",
      "Train Epoch: 1001 [2720/4897 (55%)]\tLoss: -16.952911\n",
      "Train Epoch: 1001 [2880/4897 (59%)]\tLoss: -17.722809\n",
      "Train Epoch: 1001 [3040/4897 (62%)]\tLoss: -21.459564\n",
      "Train Epoch: 1001 [3200/4897 (65%)]\tLoss: -16.601061\n",
      "Train Epoch: 1001 [3360/4897 (68%)]\tLoss: -13.085799\n",
      "Train Epoch: 1001 [3520/4897 (72%)]\tLoss: -13.211718\n",
      "Train Epoch: 1001 [3680/4897 (75%)]\tLoss: -14.818127\n",
      "Train Epoch: 1001 [3840/4897 (78%)]\tLoss: -13.321502\n",
      "Train Epoch: 1001 [4000/4897 (81%)]\tLoss: -13.611944\n",
      "Train Epoch: 1001 [4160/4897 (85%)]\tLoss: -14.159625\n",
      "Train Epoch: 1001 [4320/4897 (88%)]\tLoss: -19.038467\n",
      "Train Epoch: 1001 [4480/4897 (91%)]\tLoss: -17.139673\n",
      "Train Epoch: 1001 [4640/4897 (94%)]\tLoss: -17.661314\n",
      "Train Epoch: 1001 [4800/4897 (98%)]\tLoss: -19.612314\n",
      "====> Epoch: 1001 Average loss: -16.5916\n",
      "====> Test set loss: 17.8033\n",
      "Train Epoch: 1002 [0/4897 (0%)]\tLoss: -15.091561\n",
      "Train Epoch: 1002 [160/4897 (3%)]\tLoss: -10.595987\n",
      "Train Epoch: 1002 [320/4897 (7%)]\tLoss: -14.720861\n",
      "Train Epoch: 1002 [480/4897 (10%)]\tLoss: -11.902205\n",
      "Train Epoch: 1002 [640/4897 (13%)]\tLoss: -12.820258\n",
      "Train Epoch: 1002 [800/4897 (16%)]\tLoss: -12.692023\n",
      "Train Epoch: 1002 [960/4897 (20%)]\tLoss: -14.873493\n",
      "Train Epoch: 1002 [1120/4897 (23%)]\tLoss: -15.746045\n",
      "Train Epoch: 1002 [1280/4897 (26%)]\tLoss: -16.383465\n",
      "Train Epoch: 1002 [1440/4897 (29%)]\tLoss: -15.904099\n",
      "Train Epoch: 1002 [1600/4897 (33%)]\tLoss: -16.114941\n",
      "Train Epoch: 1002 [1760/4897 (36%)]\tLoss: -16.084543\n",
      "Train Epoch: 1002 [1920/4897 (39%)]\tLoss: -19.431459\n",
      "Train Epoch: 1002 [2080/4897 (42%)]\tLoss: -21.624016\n",
      "Train Epoch: 1002 [2240/4897 (46%)]\tLoss: -16.098051\n",
      "Train Epoch: 1002 [2400/4897 (49%)]\tLoss: -10.647653\n",
      "Train Epoch: 1002 [2560/4897 (52%)]\tLoss: -18.367708\n",
      "Train Epoch: 1002 [2720/4897 (55%)]\tLoss: -15.337653\n",
      "Train Epoch: 1002 [2880/4897 (59%)]\tLoss: -15.538855\n",
      "Train Epoch: 1002 [3040/4897 (62%)]\tLoss: -19.440037\n",
      "Train Epoch: 1002 [3200/4897 (65%)]\tLoss: -14.130795\n",
      "Train Epoch: 1002 [3360/4897 (68%)]\tLoss: -12.049834\n",
      "Train Epoch: 1002 [3520/4897 (72%)]\tLoss: -12.724718\n",
      "Train Epoch: 1002 [3680/4897 (75%)]\tLoss: -14.729885\n",
      "Train Epoch: 1002 [3840/4897 (78%)]\tLoss: -14.332653\n",
      "Train Epoch: 1002 [4000/4897 (81%)]\tLoss: -13.802352\n",
      "Train Epoch: 1002 [4160/4897 (85%)]\tLoss: -14.646626\n",
      "Train Epoch: 1002 [4320/4897 (88%)]\tLoss: -19.946548\n",
      "Train Epoch: 1002 [4480/4897 (91%)]\tLoss: -17.228130\n",
      "Train Epoch: 1002 [4640/4897 (94%)]\tLoss: -17.352615\n",
      "Train Epoch: 1002 [4800/4897 (98%)]\tLoss: -18.887016\n",
      "====> Epoch: 1002 Average loss: -15.8956\n",
      "====> Test set loss: 16.7344\n",
      "Train Epoch: 1003 [0/4897 (0%)]\tLoss: -13.803263\n",
      "Train Epoch: 1003 [160/4897 (3%)]\tLoss: -12.800214\n",
      "Train Epoch: 1003 [320/4897 (7%)]\tLoss: -16.237839\n",
      "Train Epoch: 1003 [480/4897 (10%)]\tLoss: -14.043510\n",
      "Train Epoch: 1003 [640/4897 (13%)]\tLoss: -13.926914\n",
      "Train Epoch: 1003 [800/4897 (16%)]\tLoss: -13.857088\n",
      "Train Epoch: 1003 [960/4897 (20%)]\tLoss: -16.691038\n",
      "Train Epoch: 1003 [1120/4897 (23%)]\tLoss: -18.046381\n",
      "Train Epoch: 1003 [1280/4897 (26%)]\tLoss: -19.109774\n",
      "Train Epoch: 1003 [1440/4897 (29%)]\tLoss: -17.710192\n",
      "Train Epoch: 1003 [1600/4897 (33%)]\tLoss: -16.778883\n",
      "Train Epoch: 1003 [1760/4897 (36%)]\tLoss: -15.715975\n",
      "Train Epoch: 1003 [1920/4897 (39%)]\tLoss: -17.569565\n",
      "Train Epoch: 1003 [2080/4897 (42%)]\tLoss: -18.898296\n",
      "Train Epoch: 1003 [2240/4897 (46%)]\tLoss: -14.947245\n",
      "Train Epoch: 1003 [2400/4897 (49%)]\tLoss: -11.130982\n",
      "Train Epoch: 1003 [2560/4897 (52%)]\tLoss: -17.649446\n",
      "Train Epoch: 1003 [2720/4897 (55%)]\tLoss: -15.333064\n",
      "Train Epoch: 1003 [2880/4897 (59%)]\tLoss: -16.485914\n",
      "Train Epoch: 1003 [3040/4897 (62%)]\tLoss: -20.182928\n",
      "Train Epoch: 1003 [3200/4897 (65%)]\tLoss: -15.409613\n",
      "Train Epoch: 1003 [3360/4897 (68%)]\tLoss: -14.222719\n",
      "Train Epoch: 1003 [3520/4897 (72%)]\tLoss: -14.269594\n",
      "Train Epoch: 1003 [3680/4897 (75%)]\tLoss: -16.835258\n",
      "Train Epoch: 1003 [3840/4897 (78%)]\tLoss: -14.217160\n",
      "Train Epoch: 1003 [4000/4897 (81%)]\tLoss: -11.642930\n",
      "Train Epoch: 1003 [4160/4897 (85%)]\tLoss: -12.669313\n",
      "Train Epoch: 1003 [4320/4897 (88%)]\tLoss: -16.268248\n",
      "Train Epoch: 1003 [4480/4897 (91%)]\tLoss: -14.469058\n",
      "Train Epoch: 1003 [4640/4897 (94%)]\tLoss: -14.018265\n",
      "Train Epoch: 1003 [4800/4897 (98%)]\tLoss: -17.396566\n",
      "====> Epoch: 1003 Average loss: -16.1623\n",
      "====> Test set loss: 16.0461\n",
      "Train Epoch: 1004 [0/4897 (0%)]\tLoss: -13.156905\n",
      "Train Epoch: 1004 [160/4897 (3%)]\tLoss: -13.327280\n",
      "Train Epoch: 1004 [320/4897 (7%)]\tLoss: -17.639826\n",
      "Train Epoch: 1004 [480/4897 (10%)]\tLoss: -15.709476\n",
      "Train Epoch: 1004 [640/4897 (13%)]\tLoss: -15.209991\n",
      "Train Epoch: 1004 [800/4897 (16%)]\tLoss: -14.265903\n",
      "Train Epoch: 1004 [960/4897 (20%)]\tLoss: -16.089771\n",
      "Train Epoch: 1004 [1120/4897 (23%)]\tLoss: -17.786270\n",
      "Train Epoch: 1004 [1280/4897 (26%)]\tLoss: -18.779272\n",
      "Train Epoch: 1004 [1440/4897 (29%)]\tLoss: -17.109457\n",
      "Train Epoch: 1004 [1600/4897 (33%)]\tLoss: -17.335804\n",
      "Train Epoch: 1004 [1760/4897 (36%)]\tLoss: -15.396412\n",
      "Train Epoch: 1004 [1920/4897 (39%)]\tLoss: -15.221058\n",
      "Train Epoch: 1004 [2080/4897 (42%)]\tLoss: -16.388899\n",
      "Train Epoch: 1004 [2240/4897 (46%)]\tLoss: -11.668969\n",
      "Train Epoch: 1004 [2400/4897 (49%)]\tLoss: -7.956487\n",
      "Train Epoch: 1004 [2560/4897 (52%)]\tLoss: -15.445408\n",
      "Train Epoch: 1004 [2720/4897 (55%)]\tLoss: -14.235926\n",
      "Train Epoch: 1004 [2880/4897 (59%)]\tLoss: -15.547645\n",
      "Train Epoch: 1004 [3040/4897 (62%)]\tLoss: -20.378084\n",
      "Train Epoch: 1004 [3200/4897 (65%)]\tLoss: -15.820557\n",
      "Train Epoch: 1004 [3360/4897 (68%)]\tLoss: -13.816429\n",
      "Train Epoch: 1004 [3520/4897 (72%)]\tLoss: -13.869810\n",
      "Train Epoch: 1004 [3680/4897 (75%)]\tLoss: -16.445282\n",
      "Train Epoch: 1004 [3840/4897 (78%)]\tLoss: -15.438784\n",
      "Train Epoch: 1004 [4000/4897 (81%)]\tLoss: -14.130122\n",
      "Train Epoch: 1004 [4160/4897 (85%)]\tLoss: -13.728113\n",
      "Train Epoch: 1004 [4320/4897 (88%)]\tLoss: -19.021128\n",
      "Train Epoch: 1004 [4480/4897 (91%)]\tLoss: -14.957811\n",
      "Train Epoch: 1004 [4640/4897 (94%)]\tLoss: -15.827285\n",
      "Train Epoch: 1004 [4800/4897 (98%)]\tLoss: -17.954184\n",
      "====> Epoch: 1004 Average loss: -15.9852\n",
      "====> Test set loss: 17.0311\n",
      "Train Epoch: 1005 [0/4897 (0%)]\tLoss: -14.460999\n",
      "Train Epoch: 1005 [160/4897 (3%)]\tLoss: -13.347399\n",
      "Train Epoch: 1005 [320/4897 (7%)]\tLoss: -17.133549\n",
      "Train Epoch: 1005 [480/4897 (10%)]\tLoss: -15.254068\n",
      "Train Epoch: 1005 [640/4897 (13%)]\tLoss: -15.383075\n",
      "Train Epoch: 1005 [800/4897 (16%)]\tLoss: -13.920280\n",
      "Train Epoch: 1005 [960/4897 (20%)]\tLoss: -17.628407\n",
      "Train Epoch: 1005 [1120/4897 (23%)]\tLoss: -18.160610\n",
      "Train Epoch: 1005 [1280/4897 (26%)]\tLoss: -18.221790\n",
      "Train Epoch: 1005 [1440/4897 (29%)]\tLoss: -15.665762\n",
      "Train Epoch: 1005 [1600/4897 (33%)]\tLoss: -15.375799\n",
      "Train Epoch: 1005 [1760/4897 (36%)]\tLoss: -14.683032\n",
      "Train Epoch: 1005 [1920/4897 (39%)]\tLoss: -16.376219\n",
      "Train Epoch: 1005 [2080/4897 (42%)]\tLoss: -16.184029\n",
      "Train Epoch: 1005 [2240/4897 (46%)]\tLoss: -14.421398\n",
      "Train Epoch: 1005 [2400/4897 (49%)]\tLoss: -10.256253\n",
      "Train Epoch: 1005 [2560/4897 (52%)]\tLoss: -18.391195\n",
      "Train Epoch: 1005 [2720/4897 (55%)]\tLoss: -16.259663\n",
      "Train Epoch: 1005 [2880/4897 (59%)]\tLoss: -17.718225\n",
      "Train Epoch: 1005 [3040/4897 (62%)]\tLoss: -22.087299\n",
      "Train Epoch: 1005 [3200/4897 (65%)]\tLoss: -16.803074\n",
      "Train Epoch: 1005 [3360/4897 (68%)]\tLoss: -14.492720\n",
      "Train Epoch: 1005 [3520/4897 (72%)]\tLoss: -13.879294\n",
      "Train Epoch: 1005 [3680/4897 (75%)]\tLoss: -14.474904\n",
      "Train Epoch: 1005 [3840/4897 (78%)]\tLoss: -13.508964\n",
      "Train Epoch: 1005 [4000/4897 (81%)]\tLoss: -12.579244\n",
      "Train Epoch: 1005 [4160/4897 (85%)]\tLoss: -13.380466\n",
      "Train Epoch: 1005 [4320/4897 (88%)]\tLoss: -18.839989\n",
      "Train Epoch: 1005 [4480/4897 (91%)]\tLoss: -15.494169\n",
      "Train Epoch: 1005 [4640/4897 (94%)]\tLoss: -16.510113\n",
      "Train Epoch: 1005 [4800/4897 (98%)]\tLoss: -18.482237\n",
      "====> Epoch: 1005 Average loss: -16.3069\n",
      "====> Test set loss: 15.8538\n",
      "Train Epoch: 1006 [0/4897 (0%)]\tLoss: -13.586159\n",
      "Train Epoch: 1006 [160/4897 (3%)]\tLoss: -11.290298\n",
      "Train Epoch: 1006 [320/4897 (7%)]\tLoss: -16.370296\n",
      "Train Epoch: 1006 [480/4897 (10%)]\tLoss: -14.687665\n",
      "Train Epoch: 1006 [640/4897 (13%)]\tLoss: -14.137680\n",
      "Train Epoch: 1006 [800/4897 (16%)]\tLoss: -13.292810\n",
      "Train Epoch: 1006 [960/4897 (20%)]\tLoss: -15.998530\n",
      "Train Epoch: 1006 [1120/4897 (23%)]\tLoss: -17.767693\n",
      "Train Epoch: 1006 [1280/4897 (26%)]\tLoss: -18.640764\n",
      "Train Epoch: 1006 [1440/4897 (29%)]\tLoss: -16.628283\n",
      "Train Epoch: 1006 [1600/4897 (33%)]\tLoss: -16.699045\n",
      "Train Epoch: 1006 [1760/4897 (36%)]\tLoss: -16.590746\n",
      "Train Epoch: 1006 [1920/4897 (39%)]\tLoss: -19.435175\n",
      "Train Epoch: 1006 [2080/4897 (42%)]\tLoss: -18.759703\n",
      "Train Epoch: 1006 [2240/4897 (46%)]\tLoss: -14.498650\n",
      "Train Epoch: 1006 [2400/4897 (49%)]\tLoss: -7.123788\n",
      "Train Epoch: 1006 [2560/4897 (52%)]\tLoss: -17.725321\n",
      "Train Epoch: 1006 [2720/4897 (55%)]\tLoss: -14.516482\n",
      "Train Epoch: 1006 [2880/4897 (59%)]\tLoss: -16.373507\n",
      "Train Epoch: 1006 [3040/4897 (62%)]\tLoss: -20.762304\n",
      "Train Epoch: 1006 [3200/4897 (65%)]\tLoss: -16.538820\n",
      "Train Epoch: 1006 [3360/4897 (68%)]\tLoss: -13.984752\n",
      "Train Epoch: 1006 [3520/4897 (72%)]\tLoss: -14.141745\n",
      "Train Epoch: 1006 [3680/4897 (75%)]\tLoss: -16.468502\n",
      "Train Epoch: 1006 [3840/4897 (78%)]\tLoss: -15.592815\n",
      "Train Epoch: 1006 [4000/4897 (81%)]\tLoss: -15.592644\n",
      "Train Epoch: 1006 [4160/4897 (85%)]\tLoss: -14.949455\n",
      "Train Epoch: 1006 [4320/4897 (88%)]\tLoss: -19.087887\n",
      "Train Epoch: 1006 [4480/4897 (91%)]\tLoss: -15.225741\n",
      "Train Epoch: 1006 [4640/4897 (94%)]\tLoss: -15.330170\n",
      "Train Epoch: 1006 [4800/4897 (98%)]\tLoss: -17.681362\n",
      "====> Epoch: 1006 Average loss: -16.3260\n",
      "====> Test set loss: 13.8013\n",
      "Train Epoch: 1007 [0/4897 (0%)]\tLoss: -10.907675\n",
      "Train Epoch: 1007 [160/4897 (3%)]\tLoss: -11.368464\n",
      "Train Epoch: 1007 [320/4897 (7%)]\tLoss: -15.076890\n",
      "Train Epoch: 1007 [480/4897 (10%)]\tLoss: -14.116675\n",
      "Train Epoch: 1007 [640/4897 (13%)]\tLoss: -15.091927\n",
      "Train Epoch: 1007 [800/4897 (16%)]\tLoss: -15.164318\n",
      "Train Epoch: 1007 [960/4897 (20%)]\tLoss: -17.758301\n",
      "Train Epoch: 1007 [1120/4897 (23%)]\tLoss: -18.788212\n",
      "Train Epoch: 1007 [1280/4897 (26%)]\tLoss: -18.595181\n",
      "Train Epoch: 1007 [1440/4897 (29%)]\tLoss: -17.613749\n",
      "Train Epoch: 1007 [1600/4897 (33%)]\tLoss: -17.003563\n",
      "Train Epoch: 1007 [1760/4897 (36%)]\tLoss: -15.603117\n",
      "Train Epoch: 1007 [1920/4897 (39%)]\tLoss: -17.278435\n",
      "Train Epoch: 1007 [2080/4897 (42%)]\tLoss: -17.709301\n",
      "Train Epoch: 1007 [2240/4897 (46%)]\tLoss: -12.835228\n",
      "Train Epoch: 1007 [2400/4897 (49%)]\tLoss: -9.225198\n",
      "Train Epoch: 1007 [2560/4897 (52%)]\tLoss: -17.298357\n",
      "Train Epoch: 1007 [2720/4897 (55%)]\tLoss: -16.091867\n",
      "Train Epoch: 1007 [2880/4897 (59%)]\tLoss: -17.312946\n",
      "Train Epoch: 1007 [3040/4897 (62%)]\tLoss: -22.370205\n",
      "Train Epoch: 1007 [3200/4897 (65%)]\tLoss: -17.011248\n",
      "Train Epoch: 1007 [3360/4897 (68%)]\tLoss: -14.276217\n",
      "Train Epoch: 1007 [3520/4897 (72%)]\tLoss: -13.873000\n",
      "Train Epoch: 1007 [3680/4897 (75%)]\tLoss: -16.249065\n",
      "Train Epoch: 1007 [3840/4897 (78%)]\tLoss: -15.581711\n",
      "Train Epoch: 1007 [4000/4897 (81%)]\tLoss: -12.530310\n",
      "Train Epoch: 1007 [4160/4897 (85%)]\tLoss: -12.460825\n",
      "Train Epoch: 1007 [4320/4897 (88%)]\tLoss: -16.398005\n",
      "Train Epoch: 1007 [4480/4897 (91%)]\tLoss: -12.999704\n",
      "Train Epoch: 1007 [4640/4897 (94%)]\tLoss: -15.112922\n",
      "Train Epoch: 1007 [4800/4897 (98%)]\tLoss: -17.461700\n",
      "====> Epoch: 1007 Average loss: -16.0737\n",
      "====> Test set loss: 16.7149\n",
      "Train Epoch: 1008 [0/4897 (0%)]\tLoss: -14.103936\n",
      "Train Epoch: 1008 [160/4897 (3%)]\tLoss: -13.365883\n",
      "Train Epoch: 1008 [320/4897 (7%)]\tLoss: -17.728912\n",
      "Train Epoch: 1008 [480/4897 (10%)]\tLoss: -15.851368\n",
      "Train Epoch: 1008 [640/4897 (13%)]\tLoss: -15.774162\n",
      "Train Epoch: 1008 [800/4897 (16%)]\tLoss: -14.929295\n",
      "Train Epoch: 1008 [960/4897 (20%)]\tLoss: -17.518829\n",
      "Train Epoch: 1008 [1120/4897 (23%)]\tLoss: -17.383425\n",
      "Train Epoch: 1008 [1280/4897 (26%)]\tLoss: -16.999599\n",
      "Train Epoch: 1008 [1440/4897 (29%)]\tLoss: -15.520087\n",
      "Train Epoch: 1008 [1600/4897 (33%)]\tLoss: -15.870306\n",
      "Train Epoch: 1008 [1760/4897 (36%)]\tLoss: -14.110486\n",
      "Train Epoch: 1008 [1920/4897 (39%)]\tLoss: -17.392950\n",
      "Train Epoch: 1008 [2080/4897 (42%)]\tLoss: -19.355543\n",
      "Train Epoch: 1008 [2240/4897 (46%)]\tLoss: -15.231199\n",
      "Train Epoch: 1008 [2400/4897 (49%)]\tLoss: -10.255859\n",
      "Train Epoch: 1008 [2560/4897 (52%)]\tLoss: -18.681480\n",
      "Train Epoch: 1008 [2720/4897 (55%)]\tLoss: -16.539040\n",
      "Train Epoch: 1008 [2880/4897 (59%)]\tLoss: -14.910706\n",
      "Train Epoch: 1008 [3040/4897 (62%)]\tLoss: -16.466852\n",
      "Train Epoch: 1008 [3200/4897 (65%)]\tLoss: -13.470423\n",
      "Train Epoch: 1008 [3360/4897 (68%)]\tLoss: -12.871818\n",
      "Train Epoch: 1008 [3520/4897 (72%)]\tLoss: -13.108434\n",
      "Train Epoch: 1008 [3680/4897 (75%)]\tLoss: -16.033062\n",
      "Train Epoch: 1008 [3840/4897 (78%)]\tLoss: -15.652782\n",
      "Train Epoch: 1008 [4000/4897 (81%)]\tLoss: -15.288482\n",
      "Train Epoch: 1008 [4160/4897 (85%)]\tLoss: -14.839286\n",
      "Train Epoch: 1008 [4320/4897 (88%)]\tLoss: -18.982414\n",
      "Train Epoch: 1008 [4480/4897 (91%)]\tLoss: -16.593857\n",
      "Train Epoch: 1008 [4640/4897 (94%)]\tLoss: -16.901882\n",
      "Train Epoch: 1008 [4800/4897 (98%)]\tLoss: -19.494732\n",
      "====> Epoch: 1008 Average loss: -16.5117\n",
      "====> Test set loss: 16.6430\n",
      "Train Epoch: 1009 [0/4897 (0%)]\tLoss: -13.899834\n",
      "Train Epoch: 1009 [160/4897 (3%)]\tLoss: -13.558644\n",
      "Train Epoch: 1009 [320/4897 (7%)]\tLoss: -17.558590\n",
      "Train Epoch: 1009 [480/4897 (10%)]\tLoss: -15.983246\n",
      "Train Epoch: 1009 [640/4897 (13%)]\tLoss: -15.203199\n",
      "Train Epoch: 1009 [800/4897 (16%)]\tLoss: -14.071158\n",
      "Train Epoch: 1009 [960/4897 (20%)]\tLoss: -14.693210\n",
      "Train Epoch: 1009 [1120/4897 (23%)]\tLoss: -13.502796\n",
      "Train Epoch: 1009 [1280/4897 (26%)]\tLoss: -14.503116\n",
      "Train Epoch: 1009 [1440/4897 (29%)]\tLoss: -13.963043\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Use the MIWAE and not-MIWAE on UCI data\n",
    "\"\"\"\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "#import sys\n",
    "from utils.dataframe import UCIDatasets\n",
    "from utils.experiment import exp_imputation\n",
    "#sys.path.append(os.getcwd())\n",
    "\"\"\"\n",
    "Follow the amputation setting and data settings in https://github.com/nbip/notMIWAE/blob/master/task01.py\n",
    "\"\"\"\n",
    "# ---- data settings\n",
    "name = 'whitewine'\n",
    "n_hidden = 128\n",
    "n_samples = 20\n",
    "max_iter = 100000\n",
    "batch_size = 16\n",
    "#L = 10000\n",
    "\n",
    "###   the missing model   ###\n",
    "# mprocess = 'linear'\n",
    "# mprocess = 'selfmasking'\n",
    "mprocess = 'selfmasking_known'\n",
    "\n",
    "# ---- number of runs\n",
    "runs = 1\n",
    "RMSE_result = dict()\n",
    "methods = ['miwae','notmiwae','mean','mice','RF']\n",
    "for method in methods:\n",
    "    RMSE_result[method] = []\n",
    "\"\"\"\n",
    "load data: white wine\n",
    "\"\"\"\n",
    "data = UCIDatasets(name=name)\n",
    "N, D = data.N, data.D\n",
    "dl = D - 1\n",
    "optim_kwargs = {'lr': 0.0001, 'betas': (0.9, 0.999), 'eps': 1e-08 }\n",
    "MIWAE_kwargs = {\n",
    "    'data_dim': D, 'z_dim': dl, 'h_dim': n_hidden, 'n_samples': n_samples\n",
    "    }\n",
    "notMIWAE_kwargs = {\n",
    "    'data_dim': D, 'z_dim': dl, 'h_dim': n_hidden, 'n_samples': n_samples, 'missing_process': mprocess\n",
    "    }\n",
    "data_kwargs = {\n",
    "    'batch_size': batch_size\n",
    "    }\n",
    "imputer_par = {\n",
    "    'missing_values': np.nan, 'max_iter': 10, 'random_state': 0, 'n_estimators': 100, 'n_neighbors': 3, 'metric': 'nan_euclidean'\n",
    "    }\n",
    "exp_kwargs = {\n",
    "    'dataset':name, 'runs':runs, 'seed': seed,\n",
    "}\n",
    "config = {\n",
    "    'exp_kwargs': exp_kwargs, 'optim_kwargs': optim_kwargs,\n",
    "    'MIWAE_kwargs': MIWAE_kwargs, 'notMIWAE_kwargs': notMIWAE_kwargs,\n",
    "    'data_kwargs': data_kwargs, 'imputer_par': imputer_par,\n",
    "    }\n",
    "RMSE_result = exp_imputation( 'exp_imputation', model_list = ['miwae', 'notmiwae'], config = config, num_of_epoch = max_iter)\n",
    "\n",
    "print(\"RMSE_miwae = {0:.5f} +- {1:.5f}\".format(np.mean(RMSE_result['miwae']), np.std(RMSE_result['miwae'])))\n",
    "print(\"RMSE_notmiwae = {0:.5f} +- {1:.5f}\".format(np.mean(RMSE_result['notmiwae']), np.std(RMSE_result['notmiwae'])))\n",
    "print(\"RMSE_mean = {0:.5f} +- {1:.5f}\".format(np.mean(RMSE_result['mean']), np.std(RMSE_result['mean'])))\n",
    "print(\"RMSE_mice = {0:.5f} +- {1:.5f}\".format(np.mean(RMSE_result['mice']), np.std(RMSE_result['mice'])))\n",
    "print(\"RMSE_missForest = {0:.5f} +- {1:.5f}\".format(np.mean(RMSE_result['RF']), np.std(RMSE_result['RF'])))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6db9b13bbeaa7901a248048a8ac684b541578b3064e63ac00ea1c7bbcba68952"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('pytorch': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
