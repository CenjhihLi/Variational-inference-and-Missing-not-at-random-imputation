{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font size = 6>**\n",
    "# This is the `Demo.ipynb` file\n",
    "\n",
    "I will demonstrate how to use my classes and functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
    "seed = 1\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font size = 4>**\n",
    "# The `UCIDatasets` loader in `/utils/dataframe.py `\n",
    "\n",
    "`reference: https://gist.github.com/martinferianc/db7615c85d5a3a71242b4916ea6a14a2`\n",
    "\n",
    "Note that the output `train` or `test` is pytorch.Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['housing', 'concrete', 'energy', 'power', 'redwine', 'whitewine', 'yacht']\n"
     ]
    }
   ],
   "source": [
    "from utils.dataframe import UCIDatasets, datalist\n",
    "print(datalist)\n",
    "data = UCIDatasets(\"housing\")\n",
    "train = data.get_split( load=\"train\") #pytorch.dataset\n",
    "test = data.get_split( load=\"test\")\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "train_loader = data.get_split_dataloader(load = \"train\", batch_size = 16) #pytorch.dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font size = 4>**\n",
    "# The `imputer` in `model/imputer.py`.\n",
    "\n",
    "I assemble several common methods, some detail can see my code and the documentary of sklearn.impute. \n",
    "\n",
    "I have set up some parameters. change the parameters via `par_setting( par_dict )` function.\n",
    "\n",
    "The input dictionary should be like `__getParvalue__()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mean', 'median', 'most_frequent', 'mice', 'missForest', 'knn']\n",
      "\n",
      "\n",
      "Set up parameters via impObject.par_setting( par_dict ) and retrain the model,     the par_dict should be like the following structure:\n",
      " {'missing_values': nan, 'max_iter': 10, 'random_state': 0, 'n_estimators': 100, 'n_neighbors': 3, 'metric': 'nan_euclidean'} \n",
      "\n",
      "for instance: impObject.par_setting( { 'max_iter': 10 } ) \n",
      "\n",
      "Each par corresponding to method is as the following:\n",
      "where SimpleImputer includes ['mean', 'median', 'most_frequent']\n"
     ]
    }
   ],
   "source": [
    "from model.imputer import imputer, method_list\n",
    "import numpy as np\n",
    "print(method_list)\n",
    "X = np.random.rand(100*20).reshape(100,20)\n",
    "X[51:, 11:] = np.nan\n",
    "\n",
    "impObject = imputer(X, method = 'mice')\n",
    "impObject.train()\n",
    "imp = impObject.imp\n",
    "imp.transform(X).shape \n",
    "print('\\n\\nSet up parameters via impObject.par_setting( par_dict ) and retrain the model, \\\n",
    "    the par_dict should be like the following structure:\\n {} \\n'.format(impObject.get_Parvalue()))\n",
    "print('for instance: impObject.par_setting( { \\'max_iter\\': 10 } ) ')\n",
    "print('\\nEach par corresponding to method is as the following:')\n",
    "impObject.get_Parlist()\n",
    "print('where SimpleImputer includes [\\'mean\\', \\'median\\', \\'most_frequent\\']')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font size = 4>**\n",
    "# This part is for `MIWAE` in `/model/MIWAE.py` and `trainer` in `/utils/trainer.py`.\n",
    "\n",
    "`http://proceedings.mlr.press/v97/mattei19a/mattei19a.pdf (ICML, 2019)`.\n",
    "\n",
    "`MIWAE` is a pytorch model\n",
    "\n",
    "Might use `trainer` to train it.\n",
    "\n",
    "Loss function are like `loss(self, outdic, indic)` in which outdic and indic are input and output\n",
    "\n",
    "For `MIWAE`, \n",
    "\n",
    "`indic = {'x': x , 'm': m }` where `x` is dataset and `m` is missing indicator.\n",
    "\n",
    "`output = {'lpxz': lpxz , 'lqzx': lqzx , 'lpz': lpz }`  means `l`og loss for `p`(`x` | `z`), `q`(`z` | `x`), `p`(`z`)\n",
    "\n",
    "In MIWAE loss `self.MIWAE_ELBO(outdic, indic = None)` the indic is not required.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Es torch.Size([16, 20, 20])\n",
      "Esx torch.Size([16, 20, 21])\n",
      "Esxr torch.Size([320, 21])\n",
      "h torch.Size([320, 20])\n",
      "hr torch.Size([16, 20, 20])\n",
      "hz torch.Size([16, 20, 20])\n",
      "g torch.Size([16, 20])\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                   [-1, 20]             440\n",
      "            Linear-2                  [-1, 100]           2,100\n",
      "              Tanh-3                  [-1, 100]               0\n",
      "            Linear-4                  [-1, 100]          10,100\n",
      "              Tanh-5                  [-1, 100]               0\n",
      "            Linear-6                   [-1, 50]           5,050\n",
      "            Linear-7                   [-1, 50]           5,050\n",
      "            Linear-8               [-1, 5, 100]           5,100\n",
      "            Linear-9               [-1, 5, 100]          10,100\n",
      "           Linear-10                [-1, 5, 20]           2,020\n",
      "           Linear-11                [-1, 5, 20]           2,020\n",
      "================================================================\n",
      "Total params: 41,980\n",
      "Trainable params: 41,980\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.01\n",
      "Params size (MB): 0.16\n",
      "Estimated Total Size (MB): 0.17\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from model.MIWAE import MIWAE\n",
    "from utils.trainer import VAE_trainer\n",
    "from utils.dataframe import UCIDatasets, datalist\n",
    "data = UCIDatasets(\"whitewine\")\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "train_loader = data.get_split_dataloader(load = \"train\", batch_size = 16) #pytorch.dataloader\n",
    "test_loader = data.get_split_dataloader(load = \"test\", batch_size = 16) #pytorch.dataloader\n",
    "model = MIWAE(data_dim = 20, n_samples=5, permutation_invariance=True)\n",
    "trainer = VAE_trainer(model = model, train_loader = train_loader, test_loader = test_loader)\n",
    "trainer.model_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font size = 4>**\n",
    "# A simple example\n",
    "\n",
    "Here is a simple example using `VAE_trainer` class from `/utils/trainer.py` to train `VAE` on `MNIST`.\n",
    "\n",
    "First, we may set some hyperparameters.\n",
    "\n",
    "Then read the training data and validation data (`train_loader` structure) and put them into trainer.\n",
    "\n",
    "The trainer will use `VAE_loss` automatically for `VAE` model.\n",
    "\n",
    "I will use this trainer for all `VAE` like models in this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 51 [0/60000 (0%)]\tLoss: 99.689117\n",
      "Train Epoch: 51 [1280/60000 (2%)]\tLoss: 103.431137\n",
      "Train Epoch: 51 [2560/60000 (4%)]\tLoss: 104.766655\n",
      "Train Epoch: 51 [3840/60000 (6%)]\tLoss: 99.233307\n",
      "Train Epoch: 51 [5120/60000 (9%)]\tLoss: 101.182518\n",
      "Train Epoch: 51 [6400/60000 (11%)]\tLoss: 102.850739\n",
      "Train Epoch: 51 [7680/60000 (13%)]\tLoss: 101.406624\n",
      "Train Epoch: 51 [8960/60000 (15%)]\tLoss: 100.912262\n",
      "Train Epoch: 51 [10240/60000 (17%)]\tLoss: 102.272636\n",
      "Train Epoch: 51 [11520/60000 (19%)]\tLoss: 103.237030\n",
      "Train Epoch: 51 [12800/60000 (21%)]\tLoss: 99.779877\n",
      "Train Epoch: 51 [14080/60000 (23%)]\tLoss: 100.001404\n",
      "Train Epoch: 51 [15360/60000 (26%)]\tLoss: 107.487274\n",
      "Train Epoch: 51 [16640/60000 (28%)]\tLoss: 100.886490\n",
      "Train Epoch: 51 [17920/60000 (30%)]\tLoss: 103.306786\n",
      "Train Epoch: 51 [19200/60000 (32%)]\tLoss: 100.287735\n",
      "Train Epoch: 51 [20480/60000 (34%)]\tLoss: 103.765167\n",
      "Train Epoch: 51 [21760/60000 (36%)]\tLoss: 98.166534\n",
      "Train Epoch: 51 [23040/60000 (38%)]\tLoss: 101.741867\n",
      "Train Epoch: 51 [24320/60000 (41%)]\tLoss: 99.962921\n",
      "Train Epoch: 51 [25600/60000 (43%)]\tLoss: 102.475372\n",
      "Train Epoch: 51 [26880/60000 (45%)]\tLoss: 100.807495\n",
      "Train Epoch: 51 [28160/60000 (47%)]\tLoss: 103.751839\n",
      "Train Epoch: 51 [29440/60000 (49%)]\tLoss: 101.524963\n",
      "Train Epoch: 51 [30720/60000 (51%)]\tLoss: 97.835808\n",
      "Train Epoch: 51 [32000/60000 (53%)]\tLoss: 102.529854\n",
      "Train Epoch: 51 [33280/60000 (55%)]\tLoss: 101.557816\n",
      "Train Epoch: 51 [34560/60000 (58%)]\tLoss: 103.308334\n",
      "Train Epoch: 51 [35840/60000 (60%)]\tLoss: 97.717880\n",
      "Train Epoch: 51 [37120/60000 (62%)]\tLoss: 102.811386\n",
      "Train Epoch: 51 [38400/60000 (64%)]\tLoss: 102.140579\n",
      "Train Epoch: 51 [39680/60000 (66%)]\tLoss: 98.035789\n",
      "Train Epoch: 51 [40960/60000 (68%)]\tLoss: 104.371460\n",
      "Train Epoch: 51 [42240/60000 (70%)]\tLoss: 103.376175\n",
      "Train Epoch: 51 [43520/60000 (72%)]\tLoss: 104.932373\n",
      "Train Epoch: 51 [44800/60000 (75%)]\tLoss: 101.941223\n",
      "Train Epoch: 51 [46080/60000 (77%)]\tLoss: 105.335197\n",
      "Train Epoch: 51 [47360/60000 (79%)]\tLoss: 102.158844\n",
      "Train Epoch: 51 [48640/60000 (81%)]\tLoss: 100.041145\n",
      "Train Epoch: 51 [49920/60000 (83%)]\tLoss: 100.979431\n",
      "Train Epoch: 51 [51200/60000 (85%)]\tLoss: 101.817467\n",
      "Train Epoch: 51 [52480/60000 (87%)]\tLoss: 100.590324\n",
      "Train Epoch: 51 [53760/60000 (90%)]\tLoss: 102.941391\n",
      "Train Epoch: 51 [55040/60000 (92%)]\tLoss: 98.676582\n",
      "Train Epoch: 51 [56320/60000 (94%)]\tLoss: 106.054207\n",
      "Train Epoch: 51 [57600/60000 (96%)]\tLoss: 101.895187\n",
      "Train Epoch: 51 [58880/60000 (98%)]\tLoss: 105.019066\n",
      "====> Epoch: 51 Average loss: 101.6453\n",
      "====> Test set loss: 101.7710\n",
      "Train Epoch: 52 [0/60000 (0%)]\tLoss: 102.169739\n",
      "Train Epoch: 52 [1280/60000 (2%)]\tLoss: 103.011726\n",
      "Train Epoch: 52 [2560/60000 (4%)]\tLoss: 105.368484\n",
      "Train Epoch: 52 [3840/60000 (6%)]\tLoss: 105.302391\n",
      "Train Epoch: 52 [5120/60000 (9%)]\tLoss: 104.988525\n",
      "Train Epoch: 52 [6400/60000 (11%)]\tLoss: 101.651978\n",
      "Train Epoch: 52 [7680/60000 (13%)]\tLoss: 103.728104\n",
      "Train Epoch: 52 [8960/60000 (15%)]\tLoss: 102.348160\n",
      "Train Epoch: 52 [10240/60000 (17%)]\tLoss: 104.877380\n",
      "Train Epoch: 52 [11520/60000 (19%)]\tLoss: 101.100052\n",
      "Train Epoch: 52 [12800/60000 (21%)]\tLoss: 98.888451\n",
      "Train Epoch: 52 [14080/60000 (23%)]\tLoss: 100.670212\n",
      "Train Epoch: 52 [15360/60000 (26%)]\tLoss: 99.660263\n",
      "Train Epoch: 52 [16640/60000 (28%)]\tLoss: 103.591293\n",
      "Train Epoch: 52 [17920/60000 (30%)]\tLoss: 96.341492\n",
      "Train Epoch: 52 [19200/60000 (32%)]\tLoss: 97.905128\n",
      "Train Epoch: 52 [20480/60000 (34%)]\tLoss: 101.395599\n",
      "Train Epoch: 52 [21760/60000 (36%)]\tLoss: 98.791550\n",
      "Train Epoch: 52 [23040/60000 (38%)]\tLoss: 99.417778\n",
      "Train Epoch: 52 [24320/60000 (41%)]\tLoss: 105.070679\n",
      "Train Epoch: 52 [25600/60000 (43%)]\tLoss: 103.435097\n",
      "Train Epoch: 52 [26880/60000 (45%)]\tLoss: 103.944893\n",
      "Train Epoch: 52 [28160/60000 (47%)]\tLoss: 99.101295\n",
      "Train Epoch: 52 [29440/60000 (49%)]\tLoss: 97.676903\n",
      "Train Epoch: 52 [30720/60000 (51%)]\tLoss: 101.960327\n",
      "Train Epoch: 52 [32000/60000 (53%)]\tLoss: 103.501678\n",
      "Train Epoch: 52 [33280/60000 (55%)]\tLoss: 102.770103\n",
      "Train Epoch: 52 [34560/60000 (58%)]\tLoss: 93.609131\n",
      "Train Epoch: 52 [35840/60000 (60%)]\tLoss: 103.461563\n",
      "Train Epoch: 52 [37120/60000 (62%)]\tLoss: 99.187988\n",
      "Train Epoch: 52 [38400/60000 (64%)]\tLoss: 101.221886\n",
      "Train Epoch: 52 [39680/60000 (66%)]\tLoss: 101.315292\n",
      "Train Epoch: 52 [40960/60000 (68%)]\tLoss: 103.663193\n",
      "Train Epoch: 52 [42240/60000 (70%)]\tLoss: 102.529434\n",
      "Train Epoch: 52 [43520/60000 (72%)]\tLoss: 101.679840\n",
      "Train Epoch: 52 [44800/60000 (75%)]\tLoss: 100.731308\n",
      "Train Epoch: 52 [46080/60000 (77%)]\tLoss: 101.169922\n",
      "Train Epoch: 52 [47360/60000 (79%)]\tLoss: 101.699234\n",
      "Train Epoch: 52 [48640/60000 (81%)]\tLoss: 100.461029\n",
      "Train Epoch: 52 [49920/60000 (83%)]\tLoss: 102.906021\n",
      "Train Epoch: 52 [51200/60000 (85%)]\tLoss: 101.869247\n",
      "Train Epoch: 52 [52480/60000 (87%)]\tLoss: 102.393135\n",
      "Train Epoch: 52 [53760/60000 (90%)]\tLoss: 98.207634\n",
      "Train Epoch: 52 [55040/60000 (92%)]\tLoss: 102.365883\n",
      "Train Epoch: 52 [56320/60000 (94%)]\tLoss: 101.104111\n",
      "Train Epoch: 52 [57600/60000 (96%)]\tLoss: 98.011322\n",
      "Train Epoch: 52 [58880/60000 (98%)]\tLoss: 101.931931\n",
      "====> Epoch: 52 Average loss: 101.6796\n",
      "====> Test set loss: 101.8767\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import random\n",
    "import pathlib\n",
    "import gc\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "from model.VAE import VAE\n",
    "from utils.trainer import VAE_trainer\n",
    "from utils.experiment import fs_setup, check_training_file\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
    "seed = 1\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "######################################\n",
    "# settings\n",
    "batch_size=128\n",
    "max_epochs=52\n",
    "no_cuda = False\n",
    "seed=1\n",
    "log_interval=10\n",
    "cuda = not no_cuda and torch.cuda.is_available()\n",
    "torch.manual_seed(seed)\n",
    "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "\"\"\"\n",
    "Setup directory of Demo\n",
    "\"\"\"\n",
    "config = {'batch_size': batch_size }\n",
    "experiment_dir = fs_setup('Demo', seed, config)\n",
    "expr_file = experiment_dir / f'VAE.npz'\n",
    "check_point = experiment_dir / f'VAE_ckpt.pth'\n",
    "###############\n",
    "# Read history loss file if exist\n",
    "###############\n",
    "train_file = check_training_file(expr_file)\n",
    "history, start_epoch = train_file['history'], train_file['start_epoch']\n",
    "if start_epoch >= max_epochs:\n",
    "    print('skipping {} (seed={})   start_epoch({}), num_of_epoch({})'.format('Demo', seed, start_epoch, max_epochs))\n",
    "del train_file\n",
    "gc.collect()\n",
    "###############\n",
    "# set kwargs for loading checkpoint if exist\n",
    "###############\n",
    "train_kwargs = {\n",
    "    'check_point': check_point, 'expr_file': expr_file, 'start_epoch': start_epoch, 'history': history,\n",
    "    } \n",
    "# end settings\n",
    "######################################\n",
    "\"\"\"\n",
    "Make data loader\n",
    "\"\"\"\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('./data', train=True, download=True,\n",
    "                   transform=transforms.ToTensor()),\n",
    "    batch_size=batch_size, shuffle=True, num_workers = 1, pin_memory = True )\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('./data', train=False, transform=transforms.transforms.ToTensor()),\n",
    "    batch_size=batch_size, shuffle=True, num_workers = 1, pin_memory = True)\n",
    "\"\"\"\n",
    "Train model\n",
    "\"\"\"\n",
    "model = VAE()\n",
    "trainer = VAE_trainer(model = model, train_loader = train_loader, test_loader= test_loader, \n",
    "            **config, **train_kwargs,)\n",
    "trainer.train(max_epochs = max_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font size = 4>**\n",
    "# Reproduce the experiment in the paper notMIWAE\n",
    "\n",
    "I will follow the setting in the paper https://arxiv.org/pdf/2006.12871.pdf\n",
    "\n",
    "The experiment code is in `exp_imputation` (see `utils/experiment.py`)\n",
    "\n",
    "Use `UCIDatasets` to process the data\n",
    "\n",
    "Use `imputer` to train the `mean`, `MissForest`, `MICE`\n",
    "\n",
    "Use `VAE_trainer` to train the `miwae`, `notmiwae`\n",
    "\n",
    "Store check_points every 50 epochs, including `model.state_dict()` and `optimize.state_dict()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 10151 [0/4897 (0%)]\tLoss: -18.102787\n",
      "Train Epoch: 10151 [160/4897 (3%)]\tLoss: -16.826050\n",
      "Train Epoch: 10151 [320/4897 (7%)]\tLoss: -22.099731\n",
      "Train Epoch: 10151 [480/4897 (10%)]\tLoss: -19.928675\n",
      "Train Epoch: 10151 [640/4897 (13%)]\tLoss: -20.981087\n",
      "Train Epoch: 10151 [800/4897 (16%)]\tLoss: -21.011297\n",
      "Train Epoch: 10151 [960/4897 (20%)]\tLoss: -22.922094\n",
      "Train Epoch: 10151 [1120/4897 (23%)]\tLoss: -22.524921\n",
      "Train Epoch: 10151 [1280/4897 (26%)]\tLoss: -23.510267\n",
      "Train Epoch: 10151 [1440/4897 (29%)]\tLoss: -17.749022\n",
      "Train Epoch: 10151 [1600/4897 (33%)]\tLoss: -16.829840\n",
      "Train Epoch: 10151 [1760/4897 (36%)]\tLoss: -17.391521\n",
      "Train Epoch: 10151 [1920/4897 (39%)]\tLoss: -21.467682\n",
      "Train Epoch: 10151 [2080/4897 (42%)]\tLoss: -25.092234\n",
      "Train Epoch: 10151 [2240/4897 (46%)]\tLoss: -21.024401\n",
      "Train Epoch: 10151 [2400/4897 (49%)]\tLoss: -14.670938\n",
      "Train Epoch: 10151 [2560/4897 (52%)]\tLoss: -26.113281\n",
      "Train Epoch: 10151 [2720/4897 (55%)]\tLoss: -22.954977\n",
      "Train Epoch: 10151 [2880/4897 (59%)]\tLoss: -23.322113\n",
      "Train Epoch: 10151 [3040/4897 (62%)]\tLoss: -29.306211\n",
      "Train Epoch: 10151 [3200/4897 (65%)]\tLoss: -22.214594\n",
      "Train Epoch: 10151 [3360/4897 (68%)]\tLoss: -18.851170\n",
      "Train Epoch: 10151 [3520/4897 (72%)]\tLoss: -18.926693\n",
      "Train Epoch: 10151 [3680/4897 (75%)]\tLoss: -18.665831\n",
      "Train Epoch: 10151 [3840/4897 (78%)]\tLoss: -19.508274\n",
      "Train Epoch: 10151 [4000/4897 (81%)]\tLoss: -17.371010\n",
      "Train Epoch: 10151 [4160/4897 (85%)]\tLoss: -17.823050\n",
      "Train Epoch: 10151 [4320/4897 (88%)]\tLoss: -25.574755\n",
      "Train Epoch: 10151 [4480/4897 (91%)]\tLoss: -22.349865\n",
      "Train Epoch: 10151 [4640/4897 (94%)]\tLoss: -23.024658\n",
      "Train Epoch: 10151 [4800/4897 (98%)]\tLoss: -25.868986\n",
      "====> Epoch: 10151 Average loss: -22.1649\n",
      "====> Test set loss: 23.5346\n",
      "Train Epoch: 10152 [0/4897 (0%)]\tLoss: -19.670822\n",
      "Train Epoch: 10152 [160/4897 (3%)]\tLoss: -18.395279\n",
      "Train Epoch: 10152 [320/4897 (7%)]\tLoss: -23.926149\n",
      "Train Epoch: 10152 [480/4897 (10%)]\tLoss: -21.004351\n",
      "Train Epoch: 10152 [640/4897 (13%)]\tLoss: -22.017687\n",
      "Train Epoch: 10152 [800/4897 (16%)]\tLoss: -20.532846\n",
      "Train Epoch: 10152 [960/4897 (20%)]\tLoss: -22.886055\n",
      "Train Epoch: 10152 [1120/4897 (23%)]\tLoss: -22.675472\n",
      "Train Epoch: 10152 [1280/4897 (26%)]\tLoss: -19.519760\n",
      "Train Epoch: 10152 [1440/4897 (29%)]\tLoss: -16.718887\n",
      "Train Epoch: 10152 [1600/4897 (33%)]\tLoss: -17.042345\n",
      "Train Epoch: 10152 [1760/4897 (36%)]\tLoss: -19.098597\n",
      "Train Epoch: 10152 [1920/4897 (39%)]\tLoss: -22.819590\n",
      "Train Epoch: 10152 [2080/4897 (42%)]\tLoss: -25.800440\n",
      "Train Epoch: 10152 [2240/4897 (46%)]\tLoss: -20.604425\n",
      "Train Epoch: 10152 [2400/4897 (49%)]\tLoss: -14.379898\n",
      "Train Epoch: 10152 [2560/4897 (52%)]\tLoss: -26.209881\n",
      "Train Epoch: 10152 [2720/4897 (55%)]\tLoss: -22.909327\n",
      "Train Epoch: 10152 [2880/4897 (59%)]\tLoss: -23.658318\n",
      "Train Epoch: 10152 [3040/4897 (62%)]\tLoss: -29.532000\n",
      "Train Epoch: 10152 [3200/4897 (65%)]\tLoss: -22.259989\n",
      "Train Epoch: 10152 [3360/4897 (68%)]\tLoss: -19.201038\n",
      "Train Epoch: 10152 [3520/4897 (72%)]\tLoss: -16.051069\n",
      "Train Epoch: 10152 [3680/4897 (75%)]\tLoss: -20.800009\n",
      "Train Epoch: 10152 [3840/4897 (78%)]\tLoss: -20.029268\n",
      "Train Epoch: 10152 [4000/4897 (81%)]\tLoss: -20.104496\n",
      "Train Epoch: 10152 [4160/4897 (85%)]\tLoss: -20.186668\n",
      "Train Epoch: 10152 [4320/4897 (88%)]\tLoss: -27.107670\n",
      "Train Epoch: 10152 [4480/4897 (91%)]\tLoss: -21.214073\n",
      "Train Epoch: 10152 [4640/4897 (94%)]\tLoss: -22.934488\n",
      "Train Epoch: 10152 [4800/4897 (98%)]\tLoss: -25.965078\n",
      "====> Epoch: 10152 Average loss: -22.5333\n",
      "====> Test set loss: 21.5107\n",
      "Train Epoch: 10153 [0/4897 (0%)]\tLoss: -17.823114\n",
      "Train Epoch: 10153 [160/4897 (3%)]\tLoss: 18.446390\n",
      "Train Epoch: 10153 [320/4897 (7%)]\tLoss: -17.856047\n",
      "Train Epoch: 10153 [480/4897 (10%)]\tLoss: -15.152463\n",
      "Train Epoch: 10153 [640/4897 (13%)]\tLoss: -8.608564\n",
      "Train Epoch: 10153 [800/4897 (16%)]\tLoss: -15.816192\n",
      "Train Epoch: 10153 [960/4897 (20%)]\tLoss: -19.087364\n",
      "Train Epoch: 10153 [1120/4897 (23%)]\tLoss: -19.698803\n",
      "Train Epoch: 10153 [1280/4897 (26%)]\tLoss: -20.322706\n",
      "Train Epoch: 10153 [1440/4897 (29%)]\tLoss: -18.201103\n",
      "Train Epoch: 10153 [1600/4897 (33%)]\tLoss: -19.457821\n",
      "Train Epoch: 10153 [1760/4897 (36%)]\tLoss: -18.973200\n",
      "Train Epoch: 10153 [1920/4897 (39%)]\tLoss: -23.483700\n",
      "Train Epoch: 10153 [2080/4897 (42%)]\tLoss: -25.758965\n",
      "Train Epoch: 10153 [2240/4897 (46%)]\tLoss: -20.703688\n",
      "Train Epoch: 10153 [2400/4897 (49%)]\tLoss: -14.173523\n",
      "Train Epoch: 10153 [2560/4897 (52%)]\tLoss: -25.256313\n",
      "Train Epoch: 10153 [2720/4897 (55%)]\tLoss: -22.119574\n",
      "Train Epoch: 10153 [2880/4897 (59%)]\tLoss: -22.910427\n",
      "Train Epoch: 10153 [3040/4897 (62%)]\tLoss: -28.750803\n",
      "Train Epoch: 10153 [3200/4897 (65%)]\tLoss: -21.764690\n",
      "Train Epoch: 10153 [3360/4897 (68%)]\tLoss: -18.594296\n",
      "Train Epoch: 10153 [3520/4897 (72%)]\tLoss: -18.326021\n",
      "Train Epoch: 10153 [3680/4897 (75%)]\tLoss: -21.302288\n",
      "Train Epoch: 10153 [3840/4897 (78%)]\tLoss: -18.895599\n",
      "Train Epoch: 10153 [4000/4897 (81%)]\tLoss: -19.288944\n",
      "Train Epoch: 10153 [4160/4897 (85%)]\tLoss: -19.461840\n",
      "Train Epoch: 10153 [4320/4897 (88%)]\tLoss: -25.969730\n",
      "Train Epoch: 10153 [4480/4897 (91%)]\tLoss: -21.582441\n",
      "Train Epoch: 10153 [4640/4897 (94%)]\tLoss: -22.399736\n",
      "Train Epoch: 10153 [4800/4897 (98%)]\tLoss: -25.093250\n",
      "====> Epoch: 10153 Average loss: -20.6052\n",
      "====> Test set loss: 22.6985\n",
      "Train Epoch: 10154 [0/4897 (0%)]\tLoss: -19.125778\n",
      "Train Epoch: 10154 [160/4897 (3%)]\tLoss: -16.429855\n",
      "Train Epoch: 10154 [320/4897 (7%)]\tLoss: -21.527084\n",
      "Train Epoch: 10154 [480/4897 (10%)]\tLoss: -19.205816\n",
      "Train Epoch: 10154 [640/4897 (13%)]\tLoss: -19.797489\n",
      "Train Epoch: 10154 [800/4897 (16%)]\tLoss: -19.413719\n",
      "Train Epoch: 10154 [960/4897 (20%)]\tLoss: -22.650129\n",
      "Train Epoch: 10154 [1120/4897 (23%)]\tLoss: -24.040638\n",
      "Train Epoch: 10154 [1280/4897 (26%)]\tLoss: -24.639164\n",
      "Train Epoch: 10154 [1440/4897 (29%)]\tLoss: -22.898634\n",
      "Train Epoch: 10154 [1600/4897 (33%)]\tLoss: -22.428778\n",
      "Train Epoch: 10154 [1760/4897 (36%)]\tLoss: -21.756514\n",
      "Train Epoch: 10154 [1920/4897 (39%)]\tLoss: -25.913885\n",
      "Train Epoch: 10154 [2080/4897 (42%)]\tLoss: -27.876381\n",
      "Train Epoch: 10154 [2240/4897 (46%)]\tLoss: -21.616993\n",
      "Train Epoch: 10154 [2400/4897 (49%)]\tLoss: -12.238214\n",
      "Train Epoch: 10154 [2560/4897 (52%)]\tLoss: -16.631618\n",
      "Train Epoch: 10154 [2720/4897 (55%)]\tLoss: -15.222274\n",
      "Train Epoch: 10154 [2880/4897 (59%)]\tLoss: -15.179037\n",
      "Train Epoch: 10154 [3040/4897 (62%)]\tLoss: -25.651766\n",
      "Train Epoch: 10154 [3200/4897 (65%)]\tLoss: -19.279097\n",
      "Train Epoch: 10154 [3360/4897 (68%)]\tLoss: -17.001137\n",
      "Train Epoch: 10154 [3520/4897 (72%)]\tLoss: -16.554121\n",
      "Train Epoch: 10154 [3680/4897 (75%)]\tLoss: -20.068764\n",
      "Train Epoch: 10154 [3840/4897 (78%)]\tLoss: -20.036654\n",
      "Train Epoch: 10154 [4000/4897 (81%)]\tLoss: -18.744738\n",
      "Train Epoch: 10154 [4160/4897 (85%)]\tLoss: -18.877640\n",
      "Train Epoch: 10154 [4320/4897 (88%)]\tLoss: -25.771791\n",
      "Train Epoch: 10154 [4480/4897 (91%)]\tLoss: -22.236929\n",
      "Train Epoch: 10154 [4640/4897 (94%)]\tLoss: -22.327330\n",
      "Train Epoch: 10154 [4800/4897 (98%)]\tLoss: -25.284405\n",
      "====> Epoch: 10154 Average loss: -21.4098\n",
      "====> Test set loss: 23.0525\n",
      "Train Epoch: 10155 [0/4897 (0%)]\tLoss: -19.345646\n",
      "Train Epoch: 10155 [160/4897 (3%)]\tLoss: -17.180895\n",
      "Train Epoch: 10155 [320/4897 (7%)]\tLoss: -21.568613\n",
      "Train Epoch: 10155 [480/4897 (10%)]\tLoss: -19.538982\n",
      "Train Epoch: 10155 [640/4897 (13%)]\tLoss: -20.230076\n",
      "Train Epoch: 10155 [800/4897 (16%)]\tLoss: -19.456385\n",
      "Train Epoch: 10155 [960/4897 (20%)]\tLoss: -22.341545\n",
      "Train Epoch: 10155 [1120/4897 (23%)]\tLoss: -23.704765\n",
      "Train Epoch: 10155 [1280/4897 (26%)]\tLoss: -24.591066\n",
      "Train Epoch: 10155 [1440/4897 (29%)]\tLoss: -22.870518\n",
      "Train Epoch: 10155 [1600/4897 (33%)]\tLoss: -22.245937\n",
      "Train Epoch: 10155 [1760/4897 (36%)]\tLoss: -21.385899\n",
      "Train Epoch: 10155 [1920/4897 (39%)]\tLoss: -25.802895\n",
      "Train Epoch: 10155 [2080/4897 (42%)]\tLoss: -27.513039\n",
      "Train Epoch: 10155 [2240/4897 (46%)]\tLoss: -21.573196\n",
      "Train Epoch: 10155 [2400/4897 (49%)]\tLoss: -14.755915\n",
      "Train Epoch: 10155 [2560/4897 (52%)]\tLoss: -26.382782\n",
      "Train Epoch: 10155 [2720/4897 (55%)]\tLoss: -21.536728\n",
      "Train Epoch: 10155 [2880/4897 (59%)]\tLoss: -22.805141\n",
      "Train Epoch: 10155 [3040/4897 (62%)]\tLoss: -28.278334\n",
      "Train Epoch: 10155 [3200/4897 (65%)]\tLoss: -20.372688\n",
      "Train Epoch: 10155 [3360/4897 (68%)]\tLoss: -16.148037\n",
      "Train Epoch: 10155 [3520/4897 (72%)]\tLoss: -17.405685\n",
      "Train Epoch: 10155 [3680/4897 (75%)]\tLoss: -19.725342\n",
      "Train Epoch: 10155 [3840/4897 (78%)]\tLoss: -18.749968\n",
      "Train Epoch: 10155 [4000/4897 (81%)]\tLoss: -17.910526\n",
      "Train Epoch: 10155 [4160/4897 (85%)]\tLoss: -18.954376\n",
      "Train Epoch: 10155 [4320/4897 (88%)]\tLoss: -25.262260\n",
      "Train Epoch: 10155 [4480/4897 (91%)]\tLoss: -22.587166\n",
      "Train Epoch: 10155 [4640/4897 (94%)]\tLoss: -22.334536\n",
      "Train Epoch: 10155 [4800/4897 (98%)]\tLoss: -25.045422\n",
      "====> Epoch: 10155 Average loss: -22.1051\n",
      "====> Test set loss: 22.0633\n",
      "Train Epoch: 10156 [0/4897 (0%)]\tLoss: -18.863037\n",
      "Train Epoch: 10156 [160/4897 (3%)]\tLoss: -17.099308\n",
      "Train Epoch: 10156 [320/4897 (7%)]\tLoss: -23.010448\n",
      "Train Epoch: 10156 [480/4897 (10%)]\tLoss: -20.251137\n",
      "Train Epoch: 10156 [640/4897 (13%)]\tLoss: -20.188013\n",
      "Train Epoch: 10156 [800/4897 (16%)]\tLoss: -19.402361\n",
      "Train Epoch: 10156 [960/4897 (20%)]\tLoss: -22.410219\n",
      "Train Epoch: 10156 [1120/4897 (23%)]\tLoss: -23.606562\n",
      "Train Epoch: 10156 [1280/4897 (26%)]\tLoss: -24.255728\n",
      "Train Epoch: 10156 [1440/4897 (29%)]\tLoss: -22.851252\n",
      "Train Epoch: 10156 [1600/4897 (33%)]\tLoss: -22.358519\n",
      "Train Epoch: 10156 [1760/4897 (36%)]\tLoss: -21.338659\n",
      "Train Epoch: 10156 [1920/4897 (39%)]\tLoss: -25.394415\n",
      "Train Epoch: 10156 [2080/4897 (42%)]\tLoss: -27.831131\n",
      "Train Epoch: 10156 [2240/4897 (46%)]\tLoss: -21.915335\n",
      "Train Epoch: 10156 [2400/4897 (49%)]\tLoss: -13.565831\n",
      "Train Epoch: 10156 [2560/4897 (52%)]\tLoss: -24.546059\n",
      "Train Epoch: 10156 [2720/4897 (55%)]\tLoss: -8.798916\n",
      "Train Epoch: 10156 [2880/4897 (59%)]\tLoss: -18.198221\n",
      "Train Epoch: 10156 [3040/4897 (62%)]\tLoss: -20.079372\n",
      "Train Epoch: 10156 [3200/4897 (65%)]\tLoss: -15.645140\n",
      "Train Epoch: 10156 [3360/4897 (68%)]\tLoss: -14.859392\n",
      "Train Epoch: 10156 [3520/4897 (72%)]\tLoss: -15.951954\n",
      "Train Epoch: 10156 [3680/4897 (75%)]\tLoss: -20.531965\n",
      "Train Epoch: 10156 [3840/4897 (78%)]\tLoss: -20.164549\n",
      "Train Epoch: 10156 [4000/4897 (81%)]\tLoss: -19.643656\n",
      "Train Epoch: 10156 [4160/4897 (85%)]\tLoss: -19.628143\n",
      "Train Epoch: 10156 [4320/4897 (88%)]\tLoss: -26.556143\n",
      "Train Epoch: 10156 [4480/4897 (91%)]\tLoss: -22.664577\n",
      "Train Epoch: 10156 [4640/4897 (94%)]\tLoss: -22.694794\n",
      "Train Epoch: 10156 [4800/4897 (98%)]\tLoss: -25.519373\n",
      "====> Epoch: 10156 Average loss: -21.6142\n",
      "====> Test set loss: 22.1638\n",
      "Train Epoch: 10157 [0/4897 (0%)]\tLoss: -18.884991\n",
      "Train Epoch: 10157 [160/4897 (3%)]\tLoss: -17.566345\n",
      "Train Epoch: 10157 [320/4897 (7%)]\tLoss: -23.220936\n",
      "Train Epoch: 10157 [480/4897 (10%)]\tLoss: -21.309452\n",
      "Train Epoch: 10157 [640/4897 (13%)]\tLoss: -21.417492\n",
      "Train Epoch: 10157 [800/4897 (16%)]\tLoss: -21.187550\n",
      "Train Epoch: 10157 [960/4897 (20%)]\tLoss: -23.346479\n",
      "Train Epoch: 10157 [1120/4897 (23%)]\tLoss: -16.541092\n",
      "Train Epoch: 10157 [1280/4897 (26%)]\tLoss: -22.523556\n",
      "Train Epoch: 10157 [1440/4897 (29%)]\tLoss: -20.357964\n",
      "Train Epoch: 10157 [1600/4897 (33%)]\tLoss: -20.437595\n",
      "Train Epoch: 10157 [1760/4897 (36%)]\tLoss: -18.833014\n",
      "Train Epoch: 10157 [1920/4897 (39%)]\tLoss: -18.749226\n",
      "Train Epoch: 10157 [2080/4897 (42%)]\tLoss: -24.317928\n",
      "Train Epoch: 10157 [2240/4897 (46%)]\tLoss: -19.532839\n",
      "Train Epoch: 10157 [2400/4897 (49%)]\tLoss: -13.417726\n",
      "Train Epoch: 10157 [2560/4897 (52%)]\tLoss: -23.587452\n",
      "Train Epoch: 10157 [2720/4897 (55%)]\tLoss: -21.182625\n",
      "Train Epoch: 10157 [2880/4897 (59%)]\tLoss: -22.273369\n",
      "Train Epoch: 10157 [3040/4897 (62%)]\tLoss: -28.370586\n",
      "Train Epoch: 10157 [3200/4897 (65%)]\tLoss: -21.706614\n",
      "Train Epoch: 10157 [3360/4897 (68%)]\tLoss: -18.194187\n",
      "Train Epoch: 10157 [3520/4897 (72%)]\tLoss: -18.506126\n",
      "Train Epoch: 10157 [3680/4897 (75%)]\tLoss: -21.944073\n",
      "Train Epoch: 10157 [3840/4897 (78%)]\tLoss: -20.782749\n",
      "Train Epoch: 10157 [4000/4897 (81%)]\tLoss: -20.232828\n",
      "Train Epoch: 10157 [4160/4897 (85%)]\tLoss: -19.010342\n",
      "Train Epoch: 10157 [4320/4897 (88%)]\tLoss: -26.526878\n",
      "Train Epoch: 10157 [4480/4897 (91%)]\tLoss: -22.530811\n",
      "Train Epoch: 10157 [4640/4897 (94%)]\tLoss: -22.950893\n",
      "Train Epoch: 10157 [4800/4897 (98%)]\tLoss: -25.767391\n",
      "====> Epoch: 10157 Average loss: -21.9231\n",
      "====> Test set loss: 23.2571\n",
      "Train Epoch: 10158 [0/4897 (0%)]\tLoss: -19.561459\n",
      "Train Epoch: 10158 [160/4897 (3%)]\tLoss: -18.221848\n",
      "Train Epoch: 10158 [320/4897 (7%)]\tLoss: -23.406113\n",
      "Train Epoch: 10158 [480/4897 (10%)]\tLoss: -20.983622\n",
      "Train Epoch: 10158 [640/4897 (13%)]\tLoss: -21.438440\n",
      "Train Epoch: 10158 [800/4897 (16%)]\tLoss: -21.159632\n",
      "Train Epoch: 10158 [960/4897 (20%)]\tLoss: -23.479376\n",
      "Train Epoch: 10158 [1120/4897 (23%)]\tLoss: -24.135767\n",
      "Train Epoch: 10158 [1280/4897 (26%)]\tLoss: -24.651459\n",
      "Train Epoch: 10158 [1440/4897 (29%)]\tLoss: -22.197414\n",
      "Train Epoch: 10158 [1600/4897 (33%)]\tLoss: -21.081167\n",
      "Train Epoch: 10158 [1760/4897 (36%)]\tLoss: -5.026876\n",
      "Train Epoch: 10158 [1920/4897 (39%)]\tLoss: -17.687794\n",
      "Train Epoch: 10158 [2080/4897 (42%)]\tLoss: -22.538237\n",
      "Train Epoch: 10158 [2240/4897 (46%)]\tLoss: -17.632322\n",
      "Train Epoch: 10158 [2400/4897 (49%)]\tLoss: -13.031483\n",
      "Train Epoch: 10158 [2560/4897 (52%)]\tLoss: -22.824953\n",
      "Train Epoch: 10158 [2720/4897 (55%)]\tLoss: -21.317495\n",
      "Train Epoch: 10158 [2880/4897 (59%)]\tLoss: -22.128031\n",
      "Train Epoch: 10158 [3040/4897 (62%)]\tLoss: -28.488548\n",
      "Train Epoch: 10158 [3200/4897 (65%)]\tLoss: -21.256701\n",
      "Train Epoch: 10158 [3360/4897 (68%)]\tLoss: -18.406746\n",
      "Train Epoch: 10158 [3520/4897 (72%)]\tLoss: -18.298559\n",
      "Train Epoch: 10158 [3680/4897 (75%)]\tLoss: -21.531872\n",
      "Train Epoch: 10158 [3840/4897 (78%)]\tLoss: -20.395470\n",
      "Train Epoch: 10158 [4000/4897 (81%)]\tLoss: -20.000988\n",
      "Train Epoch: 10158 [4160/4897 (85%)]\tLoss: -20.282318\n",
      "Train Epoch: 10158 [4320/4897 (88%)]\tLoss: -27.241987\n",
      "Train Epoch: 10158 [4480/4897 (91%)]\tLoss: -23.216089\n",
      "Train Epoch: 10158 [4640/4897 (94%)]\tLoss: -23.334921\n",
      "Train Epoch: 10158 [4800/4897 (98%)]\tLoss: -26.107897\n",
      "====> Epoch: 10158 Average loss: -22.0138\n",
      "====> Test set loss: 23.8517\n",
      "Train Epoch: 10159 [0/4897 (0%)]\tLoss: -19.960163\n",
      "Train Epoch: 10159 [160/4897 (3%)]\tLoss: -17.683933\n",
      "Train Epoch: 10159 [320/4897 (7%)]\tLoss: -22.619877\n",
      "Train Epoch: 10159 [480/4897 (10%)]\tLoss: -19.812105\n",
      "Train Epoch: 10159 [640/4897 (13%)]\tLoss: -20.871000\n",
      "Train Epoch: 10159 [800/4897 (16%)]\tLoss: -21.128681\n",
      "Train Epoch: 10159 [960/4897 (20%)]\tLoss: -21.876143\n",
      "Train Epoch: 10159 [1120/4897 (23%)]\tLoss: -20.549540\n",
      "Train Epoch: 10159 [1280/4897 (26%)]\tLoss: -11.861393\n",
      "Train Epoch: 10159 [1440/4897 (29%)]\tLoss: -18.504986\n",
      "Train Epoch: 10159 [1600/4897 (33%)]\tLoss: -20.241230\n",
      "Train Epoch: 10159 [1760/4897 (36%)]\tLoss: -18.809866\n",
      "Train Epoch: 10159 [1920/4897 (39%)]\tLoss: -24.126511\n",
      "Train Epoch: 10159 [2080/4897 (42%)]\tLoss: -26.333084\n",
      "Train Epoch: 10159 [2240/4897 (46%)]\tLoss: -20.998713\n",
      "Train Epoch: 10159 [2400/4897 (49%)]\tLoss: -14.586049\n",
      "Train Epoch: 10159 [2560/4897 (52%)]\tLoss: -26.086708\n",
      "Train Epoch: 10159 [2720/4897 (55%)]\tLoss: -22.879227\n",
      "Train Epoch: 10159 [2880/4897 (59%)]\tLoss: -22.654299\n",
      "Train Epoch: 10159 [3040/4897 (62%)]\tLoss: -28.472191\n",
      "Train Epoch: 10159 [3200/4897 (65%)]\tLoss: -21.896629\n",
      "Train Epoch: 10159 [3360/4897 (68%)]\tLoss: -18.377663\n",
      "Train Epoch: 10159 [3520/4897 (72%)]\tLoss: -17.799002\n",
      "Train Epoch: 10159 [3680/4897 (75%)]\tLoss: -21.708052\n",
      "Train Epoch: 10159 [3840/4897 (78%)]\tLoss: -20.814896\n",
      "Train Epoch: 10159 [4000/4897 (81%)]\tLoss: -20.195713\n",
      "Train Epoch: 10159 [4160/4897 (85%)]\tLoss: -19.905670\n",
      "Train Epoch: 10159 [4320/4897 (88%)]\tLoss: -26.935982\n",
      "Train Epoch: 10159 [4480/4897 (91%)]\tLoss: -23.172771\n",
      "Train Epoch: 10159 [4640/4897 (94%)]\tLoss: -23.142351\n",
      "Train Epoch: 10159 [4800/4897 (98%)]\tLoss: -25.552807\n",
      "====> Epoch: 10159 Average loss: -22.2801\n",
      "====> Test set loss: 22.8445\n",
      "Train Epoch: 10160 [0/4897 (0%)]\tLoss: -18.696774\n",
      "Train Epoch: 10160 [160/4897 (3%)]\tLoss: -18.352678\n",
      "Train Epoch: 10160 [320/4897 (7%)]\tLoss: -22.879957\n",
      "Train Epoch: 10160 [480/4897 (10%)]\tLoss: -15.268939\n",
      "Train Epoch: 10160 [640/4897 (13%)]\tLoss: -18.336170\n",
      "Train Epoch: 10160 [800/4897 (16%)]\tLoss: -17.935751\n",
      "Train Epoch: 10160 [960/4897 (20%)]\tLoss: -21.731836\n",
      "Train Epoch: 10160 [1120/4897 (23%)]\tLoss: -22.854784\n",
      "Train Epoch: 10160 [1280/4897 (26%)]\tLoss: -20.719656\n",
      "Train Epoch: 10160 [1440/4897 (29%)]\tLoss: -18.892200\n",
      "Train Epoch: 10160 [1600/4897 (33%)]\tLoss: -20.456589\n",
      "Train Epoch: 10160 [1760/4897 (36%)]\tLoss: -20.478640\n",
      "Train Epoch: 10160 [1920/4897 (39%)]\tLoss: -22.232592\n",
      "Train Epoch: 10160 [2080/4897 (42%)]\tLoss: -26.862642\n",
      "Train Epoch: 10160 [2240/4897 (46%)]\tLoss: -20.734694\n",
      "Train Epoch: 10160 [2400/4897 (49%)]\tLoss: -13.443402\n",
      "Train Epoch: 10160 [2560/4897 (52%)]\tLoss: -15.987671\n",
      "Train Epoch: 10160 [2720/4897 (55%)]\tLoss: -20.706490\n",
      "Train Epoch: 10160 [2880/4897 (59%)]\tLoss: -21.195738\n",
      "Train Epoch: 10160 [3040/4897 (62%)]\tLoss: -27.084045\n",
      "Train Epoch: 10160 [3200/4897 (65%)]\tLoss: -20.853298\n",
      "Train Epoch: 10160 [3360/4897 (68%)]\tLoss: -18.248760\n",
      "Train Epoch: 10160 [3520/4897 (72%)]\tLoss: -18.240086\n",
      "Train Epoch: 10160 [3680/4897 (75%)]\tLoss: -21.660341\n",
      "Train Epoch: 10160 [3840/4897 (78%)]\tLoss: -20.510395\n",
      "Train Epoch: 10160 [4000/4897 (81%)]\tLoss: -20.116592\n",
      "Train Epoch: 10160 [4160/4897 (85%)]\tLoss: -20.219875\n",
      "Train Epoch: 10160 [4320/4897 (88%)]\tLoss: -27.009747\n",
      "Train Epoch: 10160 [4480/4897 (91%)]\tLoss: -22.994015\n",
      "Train Epoch: 10160 [4640/4897 (94%)]\tLoss: -23.101398\n",
      "Train Epoch: 10160 [4800/4897 (98%)]\tLoss: -25.884581\n",
      "====> Epoch: 10160 Average loss: -21.5979\n",
      "====> Test set loss: 23.6451\n",
      "Train Epoch: 10161 [0/4897 (0%)]\tLoss: -19.859352\n",
      "Train Epoch: 10161 [160/4897 (3%)]\tLoss: -18.355530\n",
      "Train Epoch: 10161 [320/4897 (7%)]\tLoss: -23.655376\n",
      "Train Epoch: 10161 [480/4897 (10%)]\tLoss: -18.958746\n",
      "Train Epoch: 10161 [640/4897 (13%)]\tLoss: -21.401066\n",
      "Train Epoch: 10161 [800/4897 (16%)]\tLoss: -20.909510\n",
      "Train Epoch: 10161 [960/4897 (20%)]\tLoss: -21.759434\n",
      "Train Epoch: 10161 [1120/4897 (23%)]\tLoss: -19.899151\n",
      "Train Epoch: 10161 [1280/4897 (26%)]\tLoss: -13.976848\n",
      "Train Epoch: 10161 [1440/4897 (29%)]\tLoss: -19.173283\n",
      "Train Epoch: 10161 [1600/4897 (33%)]\tLoss: -18.826246\n",
      "Train Epoch: 10161 [1760/4897 (36%)]\tLoss: -18.722933\n",
      "Train Epoch: 10161 [1920/4897 (39%)]\tLoss: -23.197910\n",
      "Train Epoch: 10161 [2080/4897 (42%)]\tLoss: -25.090824\n",
      "Train Epoch: 10161 [2240/4897 (46%)]\tLoss: -20.176180\n",
      "Train Epoch: 10161 [2400/4897 (49%)]\tLoss: -13.912540\n",
      "Train Epoch: 10161 [2560/4897 (52%)]\tLoss: -25.190197\n",
      "Train Epoch: 10161 [2720/4897 (55%)]\tLoss: -22.395626\n",
      "Train Epoch: 10161 [2880/4897 (59%)]\tLoss: -22.921619\n",
      "Train Epoch: 10161 [3040/4897 (62%)]\tLoss: -28.279665\n",
      "Train Epoch: 10161 [3200/4897 (65%)]\tLoss: -21.531952\n",
      "Train Epoch: 10161 [3360/4897 (68%)]\tLoss: -18.070444\n",
      "Train Epoch: 10161 [3520/4897 (72%)]\tLoss: -18.248129\n",
      "Train Epoch: 10161 [3680/4897 (75%)]\tLoss: -21.538734\n",
      "Train Epoch: 10161 [3840/4897 (78%)]\tLoss: -20.368629\n",
      "Train Epoch: 10161 [4000/4897 (81%)]\tLoss: -20.316753\n",
      "Train Epoch: 10161 [4160/4897 (85%)]\tLoss: -16.855528\n",
      "Train Epoch: 10161 [4320/4897 (88%)]\tLoss: -26.386497\n",
      "Train Epoch: 10161 [4480/4897 (91%)]\tLoss: -21.941589\n",
      "Train Epoch: 10161 [4640/4897 (94%)]\tLoss: -22.267450\n",
      "Train Epoch: 10161 [4800/4897 (98%)]\tLoss: -24.388716\n",
      "====> Epoch: 10161 Average loss: -21.9118\n",
      "====> Test set loss: 23.1715\n",
      "Train Epoch: 10162 [0/4897 (0%)]\tLoss: -19.298571\n",
      "Train Epoch: 10162 [160/4897 (3%)]\tLoss: -17.509304\n",
      "Train Epoch: 10162 [320/4897 (7%)]\tLoss: -23.336624\n",
      "Train Epoch: 10162 [480/4897 (10%)]\tLoss: -18.749956\n",
      "Train Epoch: 10162 [640/4897 (13%)]\tLoss: -19.360512\n",
      "Train Epoch: 10162 [800/4897 (16%)]\tLoss: -14.201001\n",
      "Train Epoch: 10162 [960/4897 (20%)]\tLoss: -21.651636\n",
      "Train Epoch: 10162 [1120/4897 (23%)]\tLoss: -22.495079\n",
      "Train Epoch: 10162 [1280/4897 (26%)]\tLoss: -23.621820\n",
      "Train Epoch: 10162 [1440/4897 (29%)]\tLoss: -21.852766\n",
      "Train Epoch: 10162 [1600/4897 (33%)]\tLoss: -21.484251\n",
      "Train Epoch: 10162 [1760/4897 (36%)]\tLoss: -20.519875\n",
      "Train Epoch: 10162 [1920/4897 (39%)]\tLoss: -25.456036\n",
      "Train Epoch: 10162 [2080/4897 (42%)]\tLoss: -27.598606\n",
      "Train Epoch: 10162 [2240/4897 (46%)]\tLoss: -21.644348\n",
      "Train Epoch: 10162 [2400/4897 (49%)]\tLoss: -15.004947\n",
      "Train Epoch: 10162 [2560/4897 (52%)]\tLoss: -26.614475\n",
      "Train Epoch: 10162 [2720/4897 (55%)]\tLoss: -23.213537\n",
      "Train Epoch: 10162 [2880/4897 (59%)]\tLoss: -23.924973\n",
      "Train Epoch: 10162 [3040/4897 (62%)]\tLoss: -28.780872\n",
      "Train Epoch: 10162 [3200/4897 (65%)]\tLoss: -21.470510\n",
      "Train Epoch: 10162 [3360/4897 (68%)]\tLoss: -17.701166\n",
      "Train Epoch: 10162 [3520/4897 (72%)]\tLoss: -17.803368\n",
      "Train Epoch: 10162 [3680/4897 (75%)]\tLoss: -20.845121\n",
      "Train Epoch: 10162 [3840/4897 (78%)]\tLoss: -19.951927\n",
      "Train Epoch: 10162 [4000/4897 (81%)]\tLoss: -17.439175\n",
      "Train Epoch: 10162 [4160/4897 (85%)]\tLoss: -18.915897\n",
      "Train Epoch: 10162 [4320/4897 (88%)]\tLoss: -24.099344\n",
      "Train Epoch: 10162 [4480/4897 (91%)]\tLoss: -21.026417\n",
      "Train Epoch: 10162 [4640/4897 (94%)]\tLoss: -21.796162\n",
      "Train Epoch: 10162 [4800/4897 (98%)]\tLoss: -24.917082\n",
      "====> Epoch: 10162 Average loss: -21.7351\n",
      "====> Test set loss: 22.5823\n",
      "Train Epoch: 10163 [0/4897 (0%)]\tLoss: -18.771503\n",
      "Train Epoch: 10163 [160/4897 (3%)]\tLoss: -17.982885\n",
      "Train Epoch: 10163 [320/4897 (7%)]\tLoss: -23.776779\n",
      "Train Epoch: 10163 [480/4897 (10%)]\tLoss: -20.999254\n",
      "Train Epoch: 10163 [640/4897 (13%)]\tLoss: -20.705732\n",
      "Train Epoch: 10163 [800/4897 (16%)]\tLoss: -20.475864\n",
      "Train Epoch: 10163 [960/4897 (20%)]\tLoss: -23.009521\n",
      "Train Epoch: 10163 [1120/4897 (23%)]\tLoss: -24.316538\n",
      "Train Epoch: 10163 [1280/4897 (26%)]\tLoss: -25.333279\n",
      "Train Epoch: 10163 [1440/4897 (29%)]\tLoss: -23.317423\n",
      "Train Epoch: 10163 [1600/4897 (33%)]\tLoss: -22.737762\n",
      "Train Epoch: 10163 [1760/4897 (36%)]\tLoss: -22.132074\n",
      "Train Epoch: 10163 [1920/4897 (39%)]\tLoss: -26.358805\n",
      "Train Epoch: 10163 [2080/4897 (42%)]\tLoss: -28.483761\n",
      "Train Epoch: 10163 [2240/4897 (46%)]\tLoss: -22.256691\n",
      "Train Epoch: 10163 [2400/4897 (49%)]\tLoss: -3.947031\n",
      "Train Epoch: 10163 [2560/4897 (52%)]\tLoss: -22.161821\n",
      "Train Epoch: 10163 [2720/4897 (55%)]\tLoss: -17.934736\n",
      "Train Epoch: 10163 [2880/4897 (59%)]\tLoss: -16.864971\n",
      "Train Epoch: 10163 [3040/4897 (62%)]\tLoss: -17.963356\n",
      "Train Epoch: 10163 [3200/4897 (65%)]\tLoss: -14.518443\n",
      "Train Epoch: 10163 [3360/4897 (68%)]\tLoss: -13.803163\n",
      "Train Epoch: 10163 [3520/4897 (72%)]\tLoss: -14.310880\n",
      "Train Epoch: 10163 [3680/4897 (75%)]\tLoss: -17.371717\n",
      "Train Epoch: 10163 [3840/4897 (78%)]\tLoss: -14.628033\n",
      "Train Epoch: 10163 [4000/4897 (81%)]\tLoss: -13.637024\n",
      "Train Epoch: 10163 [4160/4897 (85%)]\tLoss: -15.828692\n",
      "Train Epoch: 10163 [4320/4897 (88%)]\tLoss: -20.349255\n",
      "Train Epoch: 10163 [4480/4897 (91%)]\tLoss: -17.549686\n",
      "Train Epoch: 10163 [4640/4897 (94%)]\tLoss: -17.481340\n",
      "Train Epoch: 10163 [4800/4897 (98%)]\tLoss: -19.762794\n",
      "====> Epoch: 10163 Average loss: -19.3785\n",
      "====> Test set loss: 18.5539\n",
      "Train Epoch: 10164 [0/4897 (0%)]\tLoss: -15.157814\n",
      "Train Epoch: 10164 [160/4897 (3%)]\tLoss: -14.846441\n",
      "Train Epoch: 10164 [320/4897 (7%)]\tLoss: -18.911306\n",
      "Train Epoch: 10164 [480/4897 (10%)]\tLoss: -17.572670\n",
      "Train Epoch: 10164 [640/4897 (13%)]\tLoss: -18.133850\n",
      "Train Epoch: 10164 [800/4897 (16%)]\tLoss: -18.910557\n",
      "Train Epoch: 10164 [960/4897 (20%)]\tLoss: -19.586853\n",
      "Train Epoch: 10164 [1120/4897 (23%)]\tLoss: -20.498253\n",
      "Train Epoch: 10164 [1280/4897 (26%)]\tLoss: -21.137857\n",
      "Train Epoch: 10164 [1440/4897 (29%)]\tLoss: -20.083626\n",
      "Train Epoch: 10164 [1600/4897 (33%)]\tLoss: -19.714489\n",
      "Train Epoch: 10164 [1760/4897 (36%)]\tLoss: -18.819759\n",
      "Train Epoch: 10164 [1920/4897 (39%)]\tLoss: -22.682064\n",
      "Train Epoch: 10164 [2080/4897 (42%)]\tLoss: -24.385756\n",
      "Train Epoch: 10164 [2240/4897 (46%)]\tLoss: -19.966267\n",
      "Train Epoch: 10164 [2400/4897 (49%)]\tLoss: -13.155930\n",
      "Train Epoch: 10164 [2560/4897 (52%)]\tLoss: -23.839252\n",
      "Train Epoch: 10164 [2720/4897 (55%)]\tLoss: -19.602978\n",
      "Train Epoch: 10164 [2880/4897 (59%)]\tLoss: -19.771198\n",
      "Train Epoch: 10164 [3040/4897 (62%)]\tLoss: -24.040970\n",
      "Train Epoch: 10164 [3200/4897 (65%)]\tLoss: -19.479252\n",
      "Train Epoch: 10164 [3360/4897 (68%)]\tLoss: -16.541351\n",
      "Train Epoch: 10164 [3520/4897 (72%)]\tLoss: -16.782362\n",
      "Train Epoch: 10164 [3680/4897 (75%)]\tLoss: -19.898361\n",
      "Train Epoch: 10164 [3840/4897 (78%)]\tLoss: -19.051180\n",
      "Train Epoch: 10164 [4000/4897 (81%)]\tLoss: -18.578245\n",
      "Train Epoch: 10164 [4160/4897 (85%)]\tLoss: -19.326267\n",
      "Train Epoch: 10164 [4320/4897 (88%)]\tLoss: -25.857042\n",
      "Train Epoch: 10164 [4480/4897 (91%)]\tLoss: -22.275036\n",
      "Train Epoch: 10164 [4640/4897 (94%)]\tLoss: -22.107492\n",
      "Train Epoch: 10164 [4800/4897 (98%)]\tLoss: -24.786366\n",
      "====> Epoch: 10164 Average loss: -20.6302\n",
      "====> Test set loss: 22.7602\n",
      "Train Epoch: 10165 [0/4897 (0%)]\tLoss: -18.903664\n",
      "Train Epoch: 10165 [160/4897 (3%)]\tLoss: -17.549114\n",
      "Train Epoch: 10165 [320/4897 (7%)]\tLoss: -22.674126\n",
      "Train Epoch: 10165 [480/4897 (10%)]\tLoss: -20.180641\n",
      "Train Epoch: 10165 [640/4897 (13%)]\tLoss: -20.549503\n",
      "Train Epoch: 10165 [800/4897 (16%)]\tLoss: -19.800430\n",
      "Train Epoch: 10165 [960/4897 (20%)]\tLoss: -22.847139\n",
      "Train Epoch: 10165 [1120/4897 (23%)]\tLoss: -23.599901\n",
      "Train Epoch: 10165 [1280/4897 (26%)]\tLoss: -22.519783\n",
      "Train Epoch: 10165 [1440/4897 (29%)]\tLoss: -21.005791\n",
      "Train Epoch: 10165 [1600/4897 (33%)]\tLoss: -20.062494\n",
      "Train Epoch: 10165 [1760/4897 (36%)]\tLoss: -19.384712\n",
      "Train Epoch: 10165 [1920/4897 (39%)]\tLoss: -21.744387\n",
      "Train Epoch: 10165 [2080/4897 (42%)]\tLoss: -21.702423\n",
      "Train Epoch: 10165 [2240/4897 (46%)]\tLoss: -19.156755\n",
      "Train Epoch: 10165 [2400/4897 (49%)]\tLoss: -11.555516\n",
      "Train Epoch: 10165 [2560/4897 (52%)]\tLoss: -23.145374\n",
      "Train Epoch: 10165 [2720/4897 (55%)]\tLoss: -20.422487\n",
      "Train Epoch: 10165 [2880/4897 (59%)]\tLoss: -20.558386\n",
      "Train Epoch: 10165 [3040/4897 (62%)]\tLoss: -26.626476\n",
      "Train Epoch: 10165 [3200/4897 (65%)]\tLoss: -20.053753\n",
      "Train Epoch: 10165 [3360/4897 (68%)]\tLoss: -16.889380\n",
      "Train Epoch: 10165 [3520/4897 (72%)]\tLoss: -17.629454\n",
      "Train Epoch: 10165 [3680/4897 (75%)]\tLoss: -21.250845\n",
      "Train Epoch: 10165 [3840/4897 (78%)]\tLoss: -20.361515\n",
      "Train Epoch: 10165 [4000/4897 (81%)]\tLoss: -19.677816\n",
      "Train Epoch: 10165 [4160/4897 (85%)]\tLoss: -18.905781\n",
      "Train Epoch: 10165 [4320/4897 (88%)]\tLoss: -26.254717\n",
      "Train Epoch: 10165 [4480/4897 (91%)]\tLoss: -22.703850\n",
      "Train Epoch: 10165 [4640/4897 (94%)]\tLoss: -22.657207\n",
      "Train Epoch: 10165 [4800/4897 (98%)]\tLoss: -25.315256\n",
      "====> Epoch: 10165 Average loss: -21.5670\n",
      "====> Test set loss: 22.4934\n",
      "Train Epoch: 10166 [0/4897 (0%)]\tLoss: -18.868977\n",
      "Train Epoch: 10166 [160/4897 (3%)]\tLoss: -17.442999\n",
      "Train Epoch: 10166 [320/4897 (7%)]\tLoss: -21.761930\n",
      "Train Epoch: 10166 [480/4897 (10%)]\tLoss: -20.377087\n",
      "Train Epoch: 10166 [640/4897 (13%)]\tLoss: -18.428055\n",
      "Train Epoch: 10166 [800/4897 (16%)]\tLoss: -18.338943\n",
      "Train Epoch: 10166 [960/4897 (20%)]\tLoss: -20.037209\n",
      "Train Epoch: 10166 [1120/4897 (23%)]\tLoss: -22.640810\n",
      "Train Epoch: 10166 [1280/4897 (26%)]\tLoss: -23.229181\n",
      "Train Epoch: 10166 [1440/4897 (29%)]\tLoss: -22.244476\n",
      "Train Epoch: 10166 [1600/4897 (33%)]\tLoss: -21.779068\n",
      "Train Epoch: 10166 [1760/4897 (36%)]\tLoss: -21.327330\n",
      "Train Epoch: 10166 [1920/4897 (39%)]\tLoss: -25.302441\n",
      "Train Epoch: 10166 [2080/4897 (42%)]\tLoss: -26.671642\n",
      "Train Epoch: 10166 [2240/4897 (46%)]\tLoss: -20.681402\n",
      "Train Epoch: 10166 [2400/4897 (49%)]\tLoss: -13.920249\n",
      "Train Epoch: 10166 [2560/4897 (52%)]\tLoss: -25.788078\n",
      "Train Epoch: 10166 [2720/4897 (55%)]\tLoss: -21.786922\n",
      "Train Epoch: 10166 [2880/4897 (59%)]\tLoss: -22.036629\n",
      "Train Epoch: 10166 [3040/4897 (62%)]\tLoss: -28.611120\n",
      "Train Epoch: 10166 [3200/4897 (65%)]\tLoss: -21.977850\n",
      "Train Epoch: 10166 [3360/4897 (68%)]\tLoss: -18.370031\n",
      "Train Epoch: 10166 [3520/4897 (72%)]\tLoss: -18.383322\n",
      "Train Epoch: 10166 [3680/4897 (75%)]\tLoss: -20.550774\n",
      "Train Epoch: 10166 [3840/4897 (78%)]\tLoss: -19.524353\n",
      "Train Epoch: 10166 [4000/4897 (81%)]\tLoss: -19.284889\n",
      "Train Epoch: 10166 [4160/4897 (85%)]\tLoss: -20.052095\n",
      "Train Epoch: 10166 [4320/4897 (88%)]\tLoss: -26.042135\n",
      "Train Epoch: 10166 [4480/4897 (91%)]\tLoss: -22.849855\n",
      "Train Epoch: 10166 [4640/4897 (94%)]\tLoss: -22.759766\n",
      "Train Epoch: 10166 [4800/4897 (98%)]\tLoss: -24.645346\n",
      "====> Epoch: 10166 Average loss: -21.9891\n",
      "====> Test set loss: 16.6726\n",
      "Train Epoch: 10167 [0/4897 (0%)]\tLoss: -15.069849\n",
      "Train Epoch: 10167 [160/4897 (3%)]\tLoss: -15.316665\n",
      "Train Epoch: 10167 [320/4897 (7%)]\tLoss: -19.071217\n",
      "Train Epoch: 10167 [480/4897 (10%)]\tLoss: -18.587082\n",
      "Train Epoch: 10167 [640/4897 (13%)]\tLoss: -18.327702\n",
      "Train Epoch: 10167 [800/4897 (16%)]\tLoss: -17.928936\n",
      "Train Epoch: 10167 [960/4897 (20%)]\tLoss: -19.854336\n",
      "Train Epoch: 10167 [1120/4897 (23%)]\tLoss: -21.861828\n",
      "Train Epoch: 10167 [1280/4897 (26%)]\tLoss: -22.914261\n",
      "Train Epoch: 10167 [1440/4897 (29%)]\tLoss: -20.474342\n",
      "Train Epoch: 10167 [1600/4897 (33%)]\tLoss: -20.241444\n",
      "Train Epoch: 10167 [1760/4897 (36%)]\tLoss: -20.065222\n",
      "Train Epoch: 10167 [1920/4897 (39%)]\tLoss: -24.049751\n",
      "Train Epoch: 10167 [2080/4897 (42%)]\tLoss: -26.095829\n",
      "Train Epoch: 10167 [2240/4897 (46%)]\tLoss: -20.979071\n",
      "Train Epoch: 10167 [2400/4897 (49%)]\tLoss: -14.546869\n",
      "Train Epoch: 10167 [2560/4897 (52%)]\tLoss: -25.920231\n",
      "Train Epoch: 10167 [2720/4897 (55%)]\tLoss: -22.447906\n",
      "Train Epoch: 10167 [2880/4897 (59%)]\tLoss: -18.612148\n",
      "Train Epoch: 10167 [3040/4897 (62%)]\tLoss: -21.963968\n",
      "Train Epoch: 10167 [3200/4897 (65%)]\tLoss: -19.548534\n",
      "Train Epoch: 10167 [3360/4897 (68%)]\tLoss: -12.146099\n",
      "Train Epoch: 10167 [3520/4897 (72%)]\tLoss: -12.565504\n",
      "Train Epoch: 10167 [3680/4897 (75%)]\tLoss: -18.833454\n",
      "Train Epoch: 10167 [3840/4897 (78%)]\tLoss: -17.747185\n",
      "Train Epoch: 10167 [4000/4897 (81%)]\tLoss: -17.801178\n",
      "Train Epoch: 10167 [4160/4897 (85%)]\tLoss: -17.489096\n",
      "Train Epoch: 10167 [4320/4897 (88%)]\tLoss: -24.447037\n",
      "Train Epoch: 10167 [4480/4897 (91%)]\tLoss: -20.139748\n",
      "Train Epoch: 10167 [4640/4897 (94%)]\tLoss: -20.754288\n",
      "Train Epoch: 10167 [4800/4897 (98%)]\tLoss: -23.736166\n",
      "====> Epoch: 10167 Average loss: -20.3827\n",
      "====> Test set loss: 20.7811\n",
      "Train Epoch: 10168 [0/4897 (0%)]\tLoss: -17.188925\n",
      "Train Epoch: 10168 [160/4897 (3%)]\tLoss: -15.135623\n",
      "Train Epoch: 10168 [320/4897 (7%)]\tLoss: -21.914915\n",
      "Train Epoch: 10168 [480/4897 (10%)]\tLoss: -20.158981\n",
      "Train Epoch: 10168 [640/4897 (13%)]\tLoss: -20.322203\n",
      "Train Epoch: 10168 [800/4897 (16%)]\tLoss: -19.584576\n",
      "Train Epoch: 10168 [960/4897 (20%)]\tLoss: -22.817371\n",
      "Train Epoch: 10168 [1120/4897 (23%)]\tLoss: -23.292286\n",
      "Train Epoch: 10168 [1280/4897 (26%)]\tLoss: -24.292685\n",
      "Train Epoch: 10168 [1440/4897 (29%)]\tLoss: -22.339218\n",
      "Train Epoch: 10168 [1600/4897 (33%)]\tLoss: -21.990177\n",
      "Train Epoch: 10168 [1760/4897 (36%)]\tLoss: -21.049616\n",
      "Train Epoch: 10168 [1920/4897 (39%)]\tLoss: -25.281096\n",
      "Train Epoch: 10168 [2080/4897 (42%)]\tLoss: -27.595793\n",
      "Train Epoch: 10168 [2240/4897 (46%)]\tLoss: -21.082375\n",
      "Train Epoch: 10168 [2400/4897 (49%)]\tLoss: -14.066695\n",
      "Train Epoch: 10168 [2560/4897 (52%)]\tLoss: -24.257946\n",
      "Train Epoch: 10168 [2720/4897 (55%)]\tLoss: -21.449099\n",
      "Train Epoch: 10168 [2880/4897 (59%)]\tLoss: -20.536491\n",
      "Train Epoch: 10168 [3040/4897 (62%)]\tLoss: -24.537197\n",
      "Train Epoch: 10168 [3200/4897 (65%)]\tLoss: -20.110418\n",
      "Train Epoch: 10168 [3360/4897 (68%)]\tLoss: -16.749722\n",
      "Train Epoch: 10168 [3520/4897 (72%)]\tLoss: -17.416920\n",
      "Train Epoch: 10168 [3680/4897 (75%)]\tLoss: -21.088694\n",
      "Train Epoch: 10168 [3840/4897 (78%)]\tLoss: -20.690197\n",
      "Train Epoch: 10168 [4000/4897 (81%)]\tLoss: -19.679560\n",
      "Train Epoch: 10168 [4160/4897 (85%)]\tLoss: -19.427273\n",
      "Train Epoch: 10168 [4320/4897 (88%)]\tLoss: -25.471720\n",
      "Train Epoch: 10168 [4480/4897 (91%)]\tLoss: -21.971601\n",
      "Train Epoch: 10168 [4640/4897 (94%)]\tLoss: -22.243317\n",
      "Train Epoch: 10168 [4800/4897 (98%)]\tLoss: -24.833448\n",
      "====> Epoch: 10168 Average loss: -21.9330\n",
      "====> Test set loss: 22.7159\n",
      "Train Epoch: 10169 [0/4897 (0%)]\tLoss: -18.851295\n",
      "Train Epoch: 10169 [160/4897 (3%)]\tLoss: -15.743063\n",
      "Train Epoch: 10169 [320/4897 (7%)]\tLoss: -21.214966\n",
      "Train Epoch: 10169 [480/4897 (10%)]\tLoss: -17.279440\n",
      "Train Epoch: 10169 [640/4897 (13%)]\tLoss: -17.246063\n",
      "Train Epoch: 10169 [800/4897 (16%)]\tLoss: -17.083914\n",
      "Train Epoch: 10169 [960/4897 (20%)]\tLoss: -20.558880\n",
      "Train Epoch: 10169 [1120/4897 (23%)]\tLoss: -20.673811\n",
      "Train Epoch: 10169 [1280/4897 (26%)]\tLoss: -21.655355\n",
      "Train Epoch: 10169 [1440/4897 (29%)]\tLoss: -20.256714\n",
      "Train Epoch: 10169 [1600/4897 (33%)]\tLoss: -20.562344\n",
      "Train Epoch: 10169 [1760/4897 (36%)]\tLoss: -20.013489\n",
      "Train Epoch: 10169 [1920/4897 (39%)]\tLoss: -24.283554\n",
      "Train Epoch: 10169 [2080/4897 (42%)]\tLoss: -26.596924\n",
      "Train Epoch: 10169 [2240/4897 (46%)]\tLoss: -21.177418\n",
      "Train Epoch: 10169 [2400/4897 (49%)]\tLoss: -14.052074\n",
      "Train Epoch: 10169 [2560/4897 (52%)]\tLoss: -25.466749\n",
      "Train Epoch: 10169 [2720/4897 (55%)]\tLoss: -22.039839\n",
      "Train Epoch: 10169 [2880/4897 (59%)]\tLoss: -22.941437\n",
      "Train Epoch: 10169 [3040/4897 (62%)]\tLoss: -29.143419\n",
      "Train Epoch: 10169 [3200/4897 (65%)]\tLoss: -21.811396\n",
      "Train Epoch: 10169 [3360/4897 (68%)]\tLoss: -18.621386\n",
      "Train Epoch: 10169 [3520/4897 (72%)]\tLoss: -18.721931\n",
      "Train Epoch: 10169 [3680/4897 (75%)]\tLoss: -22.373783\n",
      "Train Epoch: 10169 [3840/4897 (78%)]\tLoss: -20.942938\n",
      "Train Epoch: 10169 [4000/4897 (81%)]\tLoss: -20.253952\n",
      "Train Epoch: 10169 [4160/4897 (85%)]\tLoss: -15.953932\n",
      "Train Epoch: 10169 [4320/4897 (88%)]\tLoss: -22.711910\n",
      "Train Epoch: 10169 [4480/4897 (91%)]\tLoss: -15.289755\n",
      "Train Epoch: 10169 [4640/4897 (94%)]\tLoss: -19.890001\n",
      "Train Epoch: 10169 [4800/4897 (98%)]\tLoss: -21.776310\n",
      "====> Epoch: 10169 Average loss: -20.8966\n",
      "====> Test set loss: 18.8341\n",
      "Train Epoch: 10170 [0/4897 (0%)]\tLoss: -17.026375\n",
      "Train Epoch: 10170 [160/4897 (3%)]\tLoss: -14.531422\n",
      "Train Epoch: 10170 [320/4897 (7%)]\tLoss: -19.527246\n",
      "Train Epoch: 10170 [480/4897 (10%)]\tLoss: -18.712893\n",
      "Train Epoch: 10170 [640/4897 (13%)]\tLoss: -19.092720\n",
      "Train Epoch: 10170 [800/4897 (16%)]\tLoss: -19.169422\n",
      "Train Epoch: 10170 [960/4897 (20%)]\tLoss: -22.468765\n",
      "Train Epoch: 10170 [1120/4897 (23%)]\tLoss: -23.574677\n",
      "Train Epoch: 10170 [1280/4897 (26%)]\tLoss: -24.429726\n",
      "Train Epoch: 10170 [1440/4897 (29%)]\tLoss: -22.516628\n",
      "Train Epoch: 10170 [1600/4897 (33%)]\tLoss: -21.851929\n",
      "Train Epoch: 10170 [1760/4897 (36%)]\tLoss: -20.021124\n",
      "Train Epoch: 10170 [1920/4897 (39%)]\tLoss: -23.939333\n",
      "Train Epoch: 10170 [2080/4897 (42%)]\tLoss: -25.208843\n",
      "Train Epoch: 10170 [2240/4897 (46%)]\tLoss: -20.598854\n",
      "Train Epoch: 10170 [2400/4897 (49%)]\tLoss: -13.872297\n",
      "Train Epoch: 10170 [2560/4897 (52%)]\tLoss: -25.398018\n",
      "Train Epoch: 10170 [2720/4897 (55%)]\tLoss: -22.167034\n",
      "Train Epoch: 10170 [2880/4897 (59%)]\tLoss: -23.365635\n",
      "Train Epoch: 10170 [3040/4897 (62%)]\tLoss: -29.397945\n",
      "Train Epoch: 10170 [3200/4897 (65%)]\tLoss: -21.132124\n",
      "Train Epoch: 10170 [3360/4897 (68%)]\tLoss: -18.696178\n",
      "Train Epoch: 10170 [3520/4897 (72%)]\tLoss: -17.963421\n",
      "Train Epoch: 10170 [3680/4897 (75%)]\tLoss: -20.729727\n",
      "Train Epoch: 10170 [3840/4897 (78%)]\tLoss: -19.744465\n",
      "Train Epoch: 10170 [4000/4897 (81%)]\tLoss: -19.027054\n",
      "Train Epoch: 10170 [4160/4897 (85%)]\tLoss: -19.304806\n",
      "Train Epoch: 10170 [4320/4897 (88%)]\tLoss: -25.905031\n",
      "Train Epoch: 10170 [4480/4897 (91%)]\tLoss: -22.436321\n",
      "Train Epoch: 10170 [4640/4897 (94%)]\tLoss: -22.600391\n",
      "Train Epoch: 10170 [4800/4897 (98%)]\tLoss: -25.358789\n",
      "====> Epoch: 10170 Average loss: -21.9799\n",
      "====> Test set loss: 23.0284\n",
      "Train Epoch: 10171 [0/4897 (0%)]\tLoss: -19.112600\n",
      "Train Epoch: 10171 [160/4897 (3%)]\tLoss: -17.045425\n",
      "Train Epoch: 10171 [320/4897 (7%)]\tLoss: -21.253723\n",
      "Train Epoch: 10171 [480/4897 (10%)]\tLoss: -20.819193\n",
      "Train Epoch: 10171 [640/4897 (13%)]\tLoss: -21.162224\n",
      "Train Epoch: 10171 [800/4897 (16%)]\tLoss: -20.796270\n",
      "Train Epoch: 10171 [960/4897 (20%)]\tLoss: -23.389156\n",
      "Train Epoch: 10171 [1120/4897 (23%)]\tLoss: -24.572180\n",
      "Train Epoch: 10171 [1280/4897 (26%)]\tLoss: -25.352024\n",
      "Train Epoch: 10171 [1440/4897 (29%)]\tLoss: -23.369272\n",
      "Train Epoch: 10171 [1600/4897 (33%)]\tLoss: -22.659369\n",
      "Train Epoch: 10171 [1760/4897 (36%)]\tLoss: -19.335337\n",
      "Train Epoch: 10171 [1920/4897 (39%)]\tLoss: -21.995438\n",
      "Train Epoch: 10171 [2080/4897 (42%)]\tLoss: -24.551586\n",
      "Train Epoch: 10171 [2240/4897 (46%)]\tLoss: -19.920254\n",
      "Train Epoch: 10171 [2400/4897 (49%)]\tLoss: -13.577265\n",
      "Train Epoch: 10171 [2560/4897 (52%)]\tLoss: -24.412205\n",
      "Train Epoch: 10171 [2720/4897 (55%)]\tLoss: -19.857656\n",
      "Train Epoch: 10171 [2880/4897 (59%)]\tLoss: -20.174896\n",
      "Train Epoch: 10171 [3040/4897 (62%)]\tLoss: -25.246986\n",
      "Train Epoch: 10171 [3200/4897 (65%)]\tLoss: -18.411482\n",
      "Train Epoch: 10171 [3360/4897 (68%)]\tLoss: -16.649448\n",
      "Train Epoch: 10171 [3520/4897 (72%)]\tLoss: -15.743300\n",
      "Train Epoch: 10171 [3680/4897 (75%)]\tLoss: -20.358786\n",
      "Train Epoch: 10171 [3840/4897 (78%)]\tLoss: -20.001814\n",
      "Train Epoch: 10171 [4000/4897 (81%)]\tLoss: -19.435387\n",
      "Train Epoch: 10171 [4160/4897 (85%)]\tLoss: -17.679356\n",
      "Train Epoch: 10171 [4320/4897 (88%)]\tLoss: -25.831083\n",
      "Train Epoch: 10171 [4480/4897 (91%)]\tLoss: -22.100922\n",
      "Train Epoch: 10171 [4640/4897 (94%)]\tLoss: -22.527929\n",
      "Train Epoch: 10171 [4800/4897 (98%)]\tLoss: -25.406603\n",
      "====> Epoch: 10171 Average loss: -21.6038\n",
      "====> Test set loss: 23.2009\n",
      "Train Epoch: 10172 [0/4897 (0%)]\tLoss: -19.453588\n",
      "Train Epoch: 10172 [160/4897 (3%)]\tLoss: -18.180874\n",
      "Train Epoch: 10172 [320/4897 (7%)]\tLoss: -23.681936\n",
      "Train Epoch: 10172 [480/4897 (10%)]\tLoss: -19.395906\n",
      "Train Epoch: 10172 [640/4897 (13%)]\tLoss: -19.699793\n",
      "Train Epoch: 10172 [800/4897 (16%)]\tLoss: -19.828888\n",
      "Train Epoch: 10172 [960/4897 (20%)]\tLoss: -22.640579\n",
      "Train Epoch: 10172 [1120/4897 (23%)]\tLoss: -24.117815\n",
      "Train Epoch: 10172 [1280/4897 (26%)]\tLoss: -24.801659\n",
      "Train Epoch: 10172 [1440/4897 (29%)]\tLoss: -22.556711\n",
      "Train Epoch: 10172 [1600/4897 (33%)]\tLoss: -22.240707\n",
      "Train Epoch: 10172 [1760/4897 (36%)]\tLoss: -21.689676\n",
      "Train Epoch: 10172 [1920/4897 (39%)]\tLoss: -25.839439\n",
      "Train Epoch: 10172 [2080/4897 (42%)]\tLoss: -27.722946\n",
      "Train Epoch: 10172 [2240/4897 (46%)]\tLoss: -21.002468\n",
      "Train Epoch: 10172 [2400/4897 (49%)]\tLoss: -13.825867\n",
      "Train Epoch: 10172 [2560/4897 (52%)]\tLoss: -25.082855\n",
      "Train Epoch: 10172 [2720/4897 (55%)]\tLoss: -22.499498\n",
      "Train Epoch: 10172 [2880/4897 (59%)]\tLoss: -20.338261\n",
      "Train Epoch: 10172 [3040/4897 (62%)]\tLoss: -27.802303\n",
      "Train Epoch: 10172 [3200/4897 (65%)]\tLoss: -20.769016\n",
      "Train Epoch: 10172 [3360/4897 (68%)]\tLoss: -17.790562\n",
      "Train Epoch: 10172 [3520/4897 (72%)]\tLoss: -17.990242\n",
      "Train Epoch: 10172 [3680/4897 (75%)]\tLoss: -21.260439\n",
      "Train Epoch: 10172 [3840/4897 (78%)]\tLoss: -19.671028\n",
      "Train Epoch: 10172 [4000/4897 (81%)]\tLoss: -19.836315\n",
      "Train Epoch: 10172 [4160/4897 (85%)]\tLoss: -20.141327\n",
      "Train Epoch: 10172 [4320/4897 (88%)]\tLoss: -27.047531\n",
      "Train Epoch: 10172 [4480/4897 (91%)]\tLoss: -23.028904\n",
      "Train Epoch: 10172 [4640/4897 (94%)]\tLoss: -23.270298\n",
      "Train Epoch: 10172 [4800/4897 (98%)]\tLoss: -26.049137\n",
      "====> Epoch: 10172 Average loss: -22.5442\n",
      "====> Test set loss: 23.8230\n",
      "Train Epoch: 10173 [0/4897 (0%)]\tLoss: -19.951157\n",
      "Train Epoch: 10173 [160/4897 (3%)]\tLoss: -18.450790\n",
      "Train Epoch: 10173 [320/4897 (7%)]\tLoss: -22.146938\n",
      "Train Epoch: 10173 [480/4897 (10%)]\tLoss: -18.835724\n",
      "Train Epoch: 10173 [640/4897 (13%)]\tLoss: -20.211199\n",
      "Train Epoch: 10173 [800/4897 (16%)]\tLoss: -19.867912\n",
      "Train Epoch: 10173 [960/4897 (20%)]\tLoss: -22.568110\n",
      "Train Epoch: 10173 [1120/4897 (23%)]\tLoss: -23.346024\n",
      "Train Epoch: 10173 [1280/4897 (26%)]\tLoss: -24.266151\n",
      "Train Epoch: 10173 [1440/4897 (29%)]\tLoss: -22.082449\n",
      "Train Epoch: 10173 [1600/4897 (33%)]\tLoss: -22.005987\n",
      "Train Epoch: 10173 [1760/4897 (36%)]\tLoss: -18.005970\n",
      "Train Epoch: 10173 [1920/4897 (39%)]\tLoss: -23.684917\n",
      "Train Epoch: 10173 [2080/4897 (42%)]\tLoss: -26.673145\n",
      "Train Epoch: 10173 [2240/4897 (46%)]\tLoss: -21.373777\n",
      "Train Epoch: 10173 [2400/4897 (49%)]\tLoss: -12.204534\n",
      "Train Epoch: 10173 [2560/4897 (52%)]\tLoss: -23.478508\n",
      "Train Epoch: 10173 [2720/4897 (55%)]\tLoss: -20.781950\n",
      "Train Epoch: 10173 [2880/4897 (59%)]\tLoss: -21.979607\n",
      "Train Epoch: 10173 [3040/4897 (62%)]\tLoss: -27.597811\n",
      "Train Epoch: 10173 [3200/4897 (65%)]\tLoss: -20.746355\n",
      "Train Epoch: 10173 [3360/4897 (68%)]\tLoss: -17.911703\n",
      "Train Epoch: 10173 [3520/4897 (72%)]\tLoss: -18.040371\n",
      "Train Epoch: 10173 [3680/4897 (75%)]\tLoss: -21.678844\n",
      "Train Epoch: 10173 [3840/4897 (78%)]\tLoss: -16.303482\n",
      "Train Epoch: 10173 [4000/4897 (81%)]\tLoss: -19.433819\n",
      "Train Epoch: 10173 [4160/4897 (85%)]\tLoss: -17.546618\n",
      "Train Epoch: 10173 [4320/4897 (88%)]\tLoss: -24.199888\n",
      "Train Epoch: 10173 [4480/4897 (91%)]\tLoss: -21.325802\n",
      "Train Epoch: 10173 [4640/4897 (94%)]\tLoss: -20.818430\n",
      "Train Epoch: 10173 [4800/4897 (98%)]\tLoss: -24.420088\n",
      "====> Epoch: 10173 Average loss: -21.5459\n",
      "====> Test set loss: 20.8715\n",
      "Train Epoch: 10174 [0/4897 (0%)]\tLoss: -18.149923\n",
      "Train Epoch: 10174 [160/4897 (3%)]\tLoss: -16.767567\n",
      "Train Epoch: 10174 [320/4897 (7%)]\tLoss: -23.027498\n",
      "Train Epoch: 10174 [480/4897 (10%)]\tLoss: -20.396652\n",
      "Train Epoch: 10174 [640/4897 (13%)]\tLoss: -20.991034\n",
      "Train Epoch: 10174 [800/4897 (16%)]\tLoss: -20.605877\n",
      "Train Epoch: 10174 [960/4897 (20%)]\tLoss: -23.745956\n",
      "Train Epoch: 10174 [1120/4897 (23%)]\tLoss: -24.571987\n",
      "Train Epoch: 10174 [1280/4897 (26%)]\tLoss: -25.219465\n",
      "Train Epoch: 10174 [1440/4897 (29%)]\tLoss: -23.341961\n",
      "Train Epoch: 10174 [1600/4897 (33%)]\tLoss: -22.213272\n",
      "Train Epoch: 10174 [1760/4897 (36%)]\tLoss: -9.281980\n",
      "Train Epoch: 10174 [1920/4897 (39%)]\tLoss: -24.681648\n",
      "Train Epoch: 10174 [2080/4897 (42%)]\tLoss: -25.915428\n",
      "Train Epoch: 10174 [2240/4897 (46%)]\tLoss: -21.066265\n",
      "Train Epoch: 10174 [2400/4897 (49%)]\tLoss: -13.839615\n",
      "Train Epoch: 10174 [2560/4897 (52%)]\tLoss: -25.522188\n",
      "Train Epoch: 10174 [2720/4897 (55%)]\tLoss: -19.522362\n",
      "Train Epoch: 10174 [2880/4897 (59%)]\tLoss: -21.071556\n",
      "Train Epoch: 10174 [3040/4897 (62%)]\tLoss: -18.009071\n",
      "Train Epoch: 10174 [3200/4897 (65%)]\tLoss: -19.547348\n",
      "Train Epoch: 10174 [3360/4897 (68%)]\tLoss: -16.823572\n",
      "Train Epoch: 10174 [3520/4897 (72%)]\tLoss: -16.733574\n",
      "Train Epoch: 10174 [3680/4897 (75%)]\tLoss: -18.053711\n",
      "Train Epoch: 10174 [3840/4897 (78%)]\tLoss: -19.834610\n",
      "Train Epoch: 10174 [4000/4897 (81%)]\tLoss: -19.387806\n",
      "Train Epoch: 10174 [4160/4897 (85%)]\tLoss: -19.392576\n",
      "Train Epoch: 10174 [4320/4897 (88%)]\tLoss: -26.459286\n",
      "Train Epoch: 10174 [4480/4897 (91%)]\tLoss: -22.733881\n",
      "Train Epoch: 10174 [4640/4897 (94%)]\tLoss: -22.865391\n",
      "Train Epoch: 10174 [4800/4897 (98%)]\tLoss: -25.006931\n",
      "====> Epoch: 10174 Average loss: -21.7714\n",
      "====> Test set loss: 21.7794\n",
      "Train Epoch: 10175 [0/4897 (0%)]\tLoss: -18.601322\n",
      "Train Epoch: 10175 [160/4897 (3%)]\tLoss: -12.567780\n",
      "Train Epoch: 10175 [320/4897 (7%)]\tLoss: -22.199375\n",
      "Train Epoch: 10175 [480/4897 (10%)]\tLoss: -19.033932\n",
      "Train Epoch: 10175 [640/4897 (13%)]\tLoss: -19.847630\n",
      "Train Epoch: 10175 [800/4897 (16%)]\tLoss: -19.365984\n",
      "Train Epoch: 10175 [960/4897 (20%)]\tLoss: -22.201355\n",
      "Train Epoch: 10175 [1120/4897 (23%)]\tLoss: -23.573071\n",
      "Train Epoch: 10175 [1280/4897 (26%)]\tLoss: -24.585407\n",
      "Train Epoch: 10175 [1440/4897 (29%)]\tLoss: -22.820137\n",
      "Train Epoch: 10175 [1600/4897 (33%)]\tLoss: -22.519539\n",
      "Train Epoch: 10175 [1760/4897 (36%)]\tLoss: -21.877310\n",
      "Train Epoch: 10175 [1920/4897 (39%)]\tLoss: -26.010139\n",
      "Train Epoch: 10175 [2080/4897 (42%)]\tLoss: -28.164143\n",
      "Train Epoch: 10175 [2240/4897 (46%)]\tLoss: -22.037737\n",
      "Train Epoch: 10175 [2400/4897 (49%)]\tLoss: -14.990809\n",
      "Train Epoch: 10175 [2560/4897 (52%)]\tLoss: -25.940971\n",
      "Train Epoch: 10175 [2720/4897 (55%)]\tLoss: -16.674366\n",
      "Train Epoch: 10175 [2880/4897 (59%)]\tLoss: -22.370003\n",
      "Train Epoch: 10175 [3040/4897 (62%)]\tLoss: -27.494802\n",
      "Train Epoch: 10175 [3200/4897 (65%)]\tLoss: -19.832571\n",
      "Train Epoch: 10175 [3360/4897 (68%)]\tLoss: -17.557657\n",
      "Train Epoch: 10175 [3520/4897 (72%)]\tLoss: -17.626432\n",
      "Train Epoch: 10175 [3680/4897 (75%)]\tLoss: -21.083830\n",
      "Train Epoch: 10175 [3840/4897 (78%)]\tLoss: -20.080334\n",
      "Train Epoch: 10175 [4000/4897 (81%)]\tLoss: -20.114029\n",
      "Train Epoch: 10175 [4160/4897 (85%)]\tLoss: -20.215002\n",
      "Train Epoch: 10175 [4320/4897 (88%)]\tLoss: -27.193752\n",
      "Train Epoch: 10175 [4480/4897 (91%)]\tLoss: -22.978394\n",
      "Train Epoch: 10175 [4640/4897 (94%)]\tLoss: -23.102552\n",
      "Train Epoch: 10175 [4800/4897 (98%)]\tLoss: -19.705418\n",
      "====> Epoch: 10175 Average loss: -22.3598\n",
      "====> Test set loss: 19.8418\n",
      "Train Epoch: 10176 [0/4897 (0%)]\tLoss: -15.816017\n",
      "Train Epoch: 10176 [160/4897 (3%)]\tLoss: -17.507622\n",
      "Train Epoch: 10176 [320/4897 (7%)]\tLoss: -22.564367\n",
      "Train Epoch: 10176 [480/4897 (10%)]\tLoss: -19.978109\n",
      "Train Epoch: 10176 [640/4897 (13%)]\tLoss: -19.806648\n",
      "Train Epoch: 10176 [800/4897 (16%)]\tLoss: -20.786043\n",
      "Train Epoch: 10176 [960/4897 (20%)]\tLoss: -23.482914\n",
      "Train Epoch: 10176 [1120/4897 (23%)]\tLoss: -24.360857\n",
      "Train Epoch: 10176 [1280/4897 (26%)]\tLoss: -25.191252\n",
      "Train Epoch: 10176 [1440/4897 (29%)]\tLoss: -23.145636\n",
      "Train Epoch: 10176 [1600/4897 (33%)]\tLoss: -22.477036\n",
      "Train Epoch: 10176 [1760/4897 (36%)]\tLoss: -20.273491\n",
      "Train Epoch: 10176 [1920/4897 (39%)]\tLoss: -22.598856\n",
      "Train Epoch: 10176 [2080/4897 (42%)]\tLoss: -26.184120\n",
      "Train Epoch: 10176 [2240/4897 (46%)]\tLoss: -19.888666\n",
      "Train Epoch: 10176 [2400/4897 (49%)]\tLoss: -14.052563\n",
      "Train Epoch: 10176 [2560/4897 (52%)]\tLoss: -25.573940\n",
      "Train Epoch: 10176 [2720/4897 (55%)]\tLoss: -22.537212\n",
      "Train Epoch: 10176 [2880/4897 (59%)]\tLoss: -23.496315\n",
      "Train Epoch: 10176 [3040/4897 (62%)]\tLoss: -28.508831\n",
      "Train Epoch: 10176 [3200/4897 (65%)]\tLoss: -22.420116\n",
      "Train Epoch: 10176 [3360/4897 (68%)]\tLoss: -18.606239\n",
      "Train Epoch: 10176 [3520/4897 (72%)]\tLoss: -18.961224\n",
      "Train Epoch: 10176 [3680/4897 (75%)]\tLoss: -19.884541\n",
      "Train Epoch: 10176 [3840/4897 (78%)]\tLoss: -19.418074\n",
      "Train Epoch: 10176 [4000/4897 (81%)]\tLoss: -18.603285\n",
      "Train Epoch: 10176 [4160/4897 (85%)]\tLoss: -18.002148\n",
      "Train Epoch: 10176 [4320/4897 (88%)]\tLoss: -25.924778\n",
      "Train Epoch: 10176 [4480/4897 (91%)]\tLoss: -22.052525\n",
      "Train Epoch: 10176 [4640/4897 (94%)]\tLoss: -21.652548\n",
      "Train Epoch: 10176 [4800/4897 (98%)]\tLoss: -25.437943\n",
      "====> Epoch: 10176 Average loss: -22.0482\n",
      "====> Test set loss: 20.3604\n",
      "Train Epoch: 10177 [0/4897 (0%)]\tLoss: -17.622332\n",
      "Train Epoch: 10177 [160/4897 (3%)]\tLoss: -16.760981\n",
      "Train Epoch: 10177 [320/4897 (7%)]\tLoss: -21.806570\n",
      "Train Epoch: 10177 [480/4897 (10%)]\tLoss: -20.680614\n",
      "Train Epoch: 10177 [640/4897 (13%)]\tLoss: -21.110554\n",
      "Train Epoch: 10177 [800/4897 (16%)]\tLoss: -20.871578\n",
      "Train Epoch: 10177 [960/4897 (20%)]\tLoss: -23.679564\n",
      "Train Epoch: 10177 [1120/4897 (23%)]\tLoss: -24.627493\n",
      "Train Epoch: 10177 [1280/4897 (26%)]\tLoss: -25.337643\n",
      "Train Epoch: 10177 [1440/4897 (29%)]\tLoss: -23.322384\n",
      "Train Epoch: 10177 [1600/4897 (33%)]\tLoss: -21.983009\n",
      "Train Epoch: 10177 [1760/4897 (36%)]\tLoss: -20.532722\n",
      "Train Epoch: 10177 [1920/4897 (39%)]\tLoss: -24.331589\n",
      "Train Epoch: 10177 [2080/4897 (42%)]\tLoss: -26.600464\n",
      "Train Epoch: 10177 [2240/4897 (46%)]\tLoss: -21.075258\n",
      "Train Epoch: 10177 [2400/4897 (49%)]\tLoss: -14.876328\n",
      "Train Epoch: 10177 [2560/4897 (52%)]\tLoss: -26.420172\n",
      "Train Epoch: 10177 [2720/4897 (55%)]\tLoss: -23.297104\n",
      "Train Epoch: 10177 [2880/4897 (59%)]\tLoss: -21.013433\n",
      "Train Epoch: 10177 [3040/4897 (62%)]\tLoss: -24.499012\n",
      "Train Epoch: 10177 [3200/4897 (65%)]\tLoss: -20.867910\n",
      "Train Epoch: 10177 [3360/4897 (68%)]\tLoss: -17.593906\n",
      "Train Epoch: 10177 [3520/4897 (72%)]\tLoss: -17.294069\n",
      "Train Epoch: 10177 [3680/4897 (75%)]\tLoss: -20.137342\n",
      "Train Epoch: 10177 [3840/4897 (78%)]\tLoss: -19.462372\n",
      "Train Epoch: 10177 [4000/4897 (81%)]\tLoss: -18.787518\n",
      "Train Epoch: 10177 [4160/4897 (85%)]\tLoss: -17.664810\n",
      "Train Epoch: 10177 [4320/4897 (88%)]\tLoss: -26.758398\n",
      "Train Epoch: 10177 [4480/4897 (91%)]\tLoss: -22.889740\n",
      "Train Epoch: 10177 [4640/4897 (94%)]\tLoss: -23.004164\n",
      "Train Epoch: 10177 [4800/4897 (98%)]\tLoss: -25.336983\n",
      "====> Epoch: 10177 Average loss: -22.3980\n",
      "====> Test set loss: 23.1279\n",
      "Train Epoch: 10178 [0/4897 (0%)]\tLoss: -19.480097\n",
      "Train Epoch: 10178 [160/4897 (3%)]\tLoss: -17.272436\n",
      "Train Epoch: 10178 [320/4897 (7%)]\tLoss: -22.858950\n",
      "Train Epoch: 10178 [480/4897 (10%)]\tLoss: -20.281969\n",
      "Train Epoch: 10178 [640/4897 (13%)]\tLoss: -20.614002\n",
      "Train Epoch: 10178 [800/4897 (16%)]\tLoss: -19.207851\n",
      "Train Epoch: 10178 [960/4897 (20%)]\tLoss: -21.920134\n",
      "Train Epoch: 10178 [1120/4897 (23%)]\tLoss: -21.817530\n",
      "Train Epoch: 10178 [1280/4897 (26%)]\tLoss: -23.762983\n",
      "Train Epoch: 10178 [1440/4897 (29%)]\tLoss: -21.996094\n",
      "Train Epoch: 10178 [1600/4897 (33%)]\tLoss: -21.682463\n",
      "Train Epoch: 10178 [1760/4897 (36%)]\tLoss: -21.235250\n",
      "Train Epoch: 10178 [1920/4897 (39%)]\tLoss: -25.258879\n",
      "Train Epoch: 10178 [2080/4897 (42%)]\tLoss: -27.600763\n",
      "Train Epoch: 10178 [2240/4897 (46%)]\tLoss: -21.314209\n",
      "Train Epoch: 10178 [2400/4897 (49%)]\tLoss: -14.873427\n",
      "Train Epoch: 10178 [2560/4897 (52%)]\tLoss: -25.326813\n",
      "Train Epoch: 10178 [2720/4897 (55%)]\tLoss: -22.072041\n",
      "Train Epoch: 10178 [2880/4897 (59%)]\tLoss: -20.278885\n",
      "Train Epoch: 10178 [3040/4897 (62%)]\tLoss: -28.558659\n",
      "Train Epoch: 10178 [3200/4897 (65%)]\tLoss: -21.456215\n",
      "Train Epoch: 10178 [3360/4897 (68%)]\tLoss: -18.042204\n",
      "Train Epoch: 10178 [3520/4897 (72%)]\tLoss: -17.439425\n",
      "Train Epoch: 10178 [3680/4897 (75%)]\tLoss: -20.964012\n",
      "Train Epoch: 10178 [3840/4897 (78%)]\tLoss: -17.576426\n",
      "Train Epoch: 10178 [4000/4897 (81%)]\tLoss: -19.649925\n",
      "Train Epoch: 10178 [4160/4897 (85%)]\tLoss: -19.133560\n",
      "Train Epoch: 10178 [4320/4897 (88%)]\tLoss: -26.625107\n",
      "Train Epoch: 10178 [4480/4897 (91%)]\tLoss: -22.910782\n",
      "Train Epoch: 10178 [4640/4897 (94%)]\tLoss: -23.085670\n",
      "Train Epoch: 10178 [4800/4897 (98%)]\tLoss: -25.936895\n",
      "====> Epoch: 10178 Average loss: -22.1597\n",
      "====> Test set loss: 23.0440\n",
      "Train Epoch: 10179 [0/4897 (0%)]\tLoss: -19.016603\n",
      "Train Epoch: 10179 [160/4897 (3%)]\tLoss: -18.458054\n",
      "Train Epoch: 10179 [320/4897 (7%)]\tLoss: -23.735733\n",
      "Train Epoch: 10179 [480/4897 (10%)]\tLoss: -20.739222\n",
      "Train Epoch: 10179 [640/4897 (13%)]\tLoss: -21.100657\n",
      "Train Epoch: 10179 [800/4897 (16%)]\tLoss: -20.713499\n",
      "Train Epoch: 10179 [960/4897 (20%)]\tLoss: -23.611250\n",
      "Train Epoch: 10179 [1120/4897 (23%)]\tLoss: -23.343079\n",
      "Train Epoch: 10179 [1280/4897 (26%)]\tLoss: -25.045071\n",
      "Train Epoch: 10179 [1440/4897 (29%)]\tLoss: -22.535389\n",
      "Train Epoch: 10179 [1600/4897 (33%)]\tLoss: -22.488953\n",
      "Train Epoch: 10179 [1760/4897 (36%)]\tLoss: -20.869181\n",
      "Train Epoch: 10179 [1920/4897 (39%)]\tLoss: -25.680212\n",
      "Train Epoch: 10179 [2080/4897 (42%)]\tLoss: -24.489058\n",
      "Train Epoch: 10179 [2240/4897 (46%)]\tLoss: -20.302240\n",
      "Train Epoch: 10179 [2400/4897 (49%)]\tLoss: -11.046901\n",
      "Train Epoch: 10179 [2560/4897 (52%)]\tLoss: 25.742931\n",
      "Train Epoch: 10179 [2720/4897 (55%)]\tLoss: -12.511582\n",
      "Train Epoch: 10179 [2880/4897 (59%)]\tLoss: -15.300690\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected parameter loc (Tensor of shape (16, 10)) of distribution Normal(loc: torch.Size([16, 10]), scale: torch.Size([16, 10])) to satisfy the constraint Real(), but found invalid values:\ntensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n       grad_fn=<AddmmBackward0>)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_29008/3396600425.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;34m'data_kwargs'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mdata_kwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'imputer_par'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mimputer_par\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m     }\n\u001b[1;32m---> 61\u001b[1;33m \u001b[0mRMSE_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexp_imputation\u001b[0m\u001b[1;33m(\u001b[0m \u001b[1;34m'exp_imputation'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'miwae'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'notmiwae'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_of_epoch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"RMSE_miwae = {0:.5f} +- {1:.5f}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mRMSE_result\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'miwae'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mRMSE_result\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'miwae'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\GitHub\\Variational inference and Missing not at random\\utils\\experiment.py\u001b[0m in \u001b[0;36mexp_imputation\u001b[1;34m(experiment_name, model_list, config, num_of_epoch, CUDA_VISIBLE_DEVICES)\u001b[0m\n\u001b[0;32m    157\u001b[0m                                 \u001b[1;33m**\u001b[0m\u001b[0mdata_kwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mtrain_kwargs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    158\u001b[0m                                 optim_kwargs = optim_kwargs) \n\u001b[1;32m--> 159\u001b[1;33m             \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_epochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_of_epoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    160\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    161\u001b[0m             \u001b[1;31m# imputation RMSE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\GitHub\\Variational inference and Missing not at random\\utils\\trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, max_epochs)\u001b[0m\n\u001b[0;32m    188\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart_epoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_epochs\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 190\u001b[1;33m             \u001b[0mtrain_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_batch_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    191\u001b[0m             \u001b[0mval_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\GitHub\\Variational inference and Missing not at random\\utils\\trainer.py\u001b[0m in \u001b[0;36m_batch_train\u001b[1;34m(self, epoch)\u001b[0m\n\u001b[0;32m    131\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 133\u001b[1;33m             \u001b[0moutdic\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mq_z\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout_sample\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    134\u001b[0m             \u001b[1;31m#in VAE, z is sample from q_z, then obtain the ouput recon_x by decode(z)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m             \u001b[1;31m#but in MIWAE, z is sample from p_x_given_z\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\GitHub\\Variational inference and Missing not at random\\model\\MIWAE.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x, n_samples)\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m         \u001b[1;31m# sample latent values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 211\u001b[1;33m         \u001b[0mq_z\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml_z\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreparameterize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mq_mu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mq_log_sig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    212\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m         \"\"\"\n",
      "\u001b[1;32mc:\\GitHub\\Variational inference and Missing not at random\\model\\MIWAE.py\u001b[0m in \u001b[0;36mreparameterize\u001b[1;34m(self, mu, log_var, n_samples)\u001b[0m\n\u001b[0;32m    125\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mreparameterize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog_var\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m         \u001b[0mstd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlog_var\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 127\u001b[1;33m         \u001b[0mq_z\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpdfun\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNormal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    128\u001b[0m         \u001b[1;31m# q_z.rsample(self.n_samples): shape [n_samples, batch_size, d]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m         \u001b[1;31m#           .permute(1, 0, 2): shape [batch_size, n_samples, d]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\pytorch\\lib\\site-packages\\torch\\distributions\\normal.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, loc, scale, validate_args)\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m             \u001b[0mbatch_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mNormal\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_shape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidate_args\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mexpand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_shape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_instance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\pytorch\\lib\\site-packages\\torch\\distributions\\distribution.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch_shape, event_shape, validate_args)\u001b[0m\n\u001b[0;32m     53\u001b[0m                 \u001b[0mvalid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconstraint\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mvalid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m                     raise ValueError(\n\u001b[0m\u001b[0;32m     56\u001b[0m                         \u001b[1;34mf\"Expected parameter {param} \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m                         \u001b[1;34mf\"({type(value).__name__} of shape {tuple(value.shape)}) \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Expected parameter loc (Tensor of shape (16, 10)) of distribution Normal(loc: torch.Size([16, 10]), scale: torch.Size([16, 10])) to satisfy the constraint Real(), but found invalid values:\ntensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n       grad_fn=<AddmmBackward0>)"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Use the MIWAE and not-MIWAE on UCI data\n",
    "\"\"\"\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "#import sys\n",
    "from utils.dataframe import UCIDatasets\n",
    "from utils.experiment import exp_imputation\n",
    "#sys.path.append(os.getcwd())\n",
    "\"\"\"\n",
    "Follow the amputation setting and data settings in https://github.com/nbip/notMIWAE/blob/master/task01.py\n",
    "\"\"\"\n",
    "# ---- data settings\n",
    "name = 'whitewine'\n",
    "n_hidden = 128\n",
    "n_samples = 20\n",
    "max_iter = 100000\n",
    "batch_size = 16\n",
    "impute_sample = 10000\n",
    "\n",
    "###   the missing model   ###\n",
    "# mprocess = 'linear'\n",
    "# mprocess = 'selfmasking'\n",
    "mprocess = 'selfmasking_known'\n",
    "\n",
    "# ---- number of runs\n",
    "runs = 1\n",
    "RMSE_result = dict()\n",
    "methods = ['miwae','notmiwae','mean','mice','RF']\n",
    "for method in methods:\n",
    "    RMSE_result[method] = []\n",
    "\"\"\"\n",
    "load data: white wine\n",
    "\"\"\"\n",
    "data = UCIDatasets(name=name)\n",
    "N, D = data.N, data.D\n",
    "dl = D - 1\n",
    "optim_kwargs = {'lr': 0.0001, 'betas': (0.9, 0.999), 'eps': 1e-08 }\n",
    "MIWAE_kwargs = {\n",
    "    'data_dim': D, 'z_dim': dl, 'h_dim': n_hidden, 'n_samples': n_samples\n",
    "    }\n",
    "notMIWAE_kwargs = {\n",
    "    'data_dim': D, 'z_dim': dl, 'h_dim': n_hidden, 'n_samples': n_samples, 'missing_process': mprocess\n",
    "    }\n",
    "data_kwargs = {\n",
    "    'batch_size': batch_size\n",
    "    }\n",
    "imputer_par = {\n",
    "    'missing_values': np.nan, 'max_iter': 10, 'random_state': 0, 'n_estimators': 100, 'n_neighbors': 3, 'metric': 'nan_euclidean'\n",
    "    }\n",
    "exp_kwargs = {\n",
    "    'dataset':name, 'runs':runs, 'seed': seed, 'impute_sample': impute_sample,\n",
    "}\n",
    "config = {\n",
    "    'exp_kwargs': exp_kwargs, 'optim_kwargs': optim_kwargs,\n",
    "    'MIWAE_kwargs': MIWAE_kwargs, 'notMIWAE_kwargs': notMIWAE_kwargs,\n",
    "    'data_kwargs': data_kwargs, 'imputer_par': imputer_par,\n",
    "    }\n",
    "RMSE_result = exp_imputation( 'exp_imputation', model_list = ['miwae', 'notmiwae'], config = config, num_of_epoch = max_iter)\n",
    "\n",
    "print(\"RMSE_miwae = {0:.5f} +- {1:.5f}\".format(np.mean(RMSE_result['miwae']), np.std(RMSE_result['miwae'])))\n",
    "print(\"RMSE_notmiwae = {0:.5f} +- {1:.5f}\".format(np.mean(RMSE_result['notmiwae']), np.std(RMSE_result['notmiwae'])))\n",
    "print(\"RMSE_mean = {0:.5f} +- {1:.5f}\".format(np.mean(RMSE_result['mean']), np.std(RMSE_result['mean'])))\n",
    "print(\"RMSE_mice = {0:.5f} +- {1:.5f}\".format(np.mean(RMSE_result['mice']), np.std(RMSE_result['mice'])))\n",
    "print(\"RMSE_missForest = {0:.5f} +- {1:.5f}\".format(np.mean(RMSE_result['RF']), np.std(RMSE_result['RF'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font size = 4>**\n",
    "# Some issues here\n",
    "\n",
    "I follow all settings from the paper, a big difference is I use pytorch and the original paper use tensorflow 1\n",
    "\n",
    "The encoder seems has some problem (need to fix it)\n",
    "\n",
    "The original paper use clip as activation function for std in the encoder.\n",
    "\n",
    "Perhaps torch.clamp has a little difference I haven't noticed."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6db9b13bbeaa7901a248048a8ac684b541578b3064e63ac00ea1c7bbcba68952"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('pytorch': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
